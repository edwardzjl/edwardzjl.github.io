"use strict";(self.webpackChunkedwardzjl_github_io=self.webpackChunkedwardzjl_github_io||[]).push([[518],{4369:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"distributed-sft-part-2","metadata":{"permalink":"/distributed-sft-part-2","editUrl":"https://github.com/edwardzjl/edwardzjl.github.io/blob/main/blog/2025-02-07-distributed-sft-part-2/index.md","source":"@site/blog/2025-02-07-distributed-sft-part-2/index.md","title":"Distributed SFT Part 2: Scaling Locally","description":"Introduction","date":"2025-02-07T00:00:00.000Z","tags":[{"inline":true,"label":"LLM","permalink":"/tags/llm"},{"inline":true,"label":"distributed-training","permalink":"/tags/distributed-training"}],"readingTime":12.72,"hasTruncateMarker":true,"authors":[{"name":"Junlin Zhou","title":"Fullstack Engineer @ ZJU ICI","url":"https://github.com/edwardzjl","imageURL":"https://github.com/edwardzjl.png","key":"jlzhou","page":null}],"frontMatter":{"slug":"distributed-sft-part-2","authors":["jlzhou"],"tags":["LLM","distributed-training"]},"unlisted":false,"nextItem":{"title":"Distributed SFT Part 1: Starting Locally","permalink":"/distributed-sft-part-1"}},"content":"## Introduction\\n\\n[In the first part of this series](https://edwardzjl.github.io/distributed-sft-part-1), we covered the basics of setting up a local SFT experiment using `trl`. We learned how to format datasets for `trl`\'s `SFTTrainer` and preprocess them to fit the required structure.\\n\\nNow, it\'s time to take the next step. In this post, we\'ll focus on scaling the SFT setup to handle larger tasks. Specifically, we\'ll explore how to fine-tune an LLM in a single-node, multi-GPU environment. Along the way, we\'ll discuss optimization techniques to reduce memory usage, speed up training, and enable fine-tuning of even larger models. Let\'s get started!\\n\\n\x3c!-- truncate --\x3e\\n\\n## Prerequisites\\n\\nTo follow along with this tutorial, you\'ll need a machine equipped with multiple NVIDIA GPUs. Ensure that the GPUs are connected via high-speed interconnects to minimize communication overhead. For reference, I ran this experiment using 8 NVIDIA V100 SXM2 GPUs.\\n\\n**Important Considerations:**\\n\\n1. GPU Architecture: While I ran this experiment with V100 GPUs, newer architectures like Ampere or Hopper are strongly recommended. These GPUs offers advanced features, such as support for more efficient precision types and improved communication speeds. Additionally, techniques like [flash-attention](https://github.com/Dao-AILab/flash-attention) are [only compatible with Ampere or newer GPUs](https://github.com/Dao-AILab/flash-attention/issues/524).\\n\\n2. Interconnect Quality: Verify GPU communication bandwidth using `nvidia-smi topo -m`. Poor interconnects can become a bottleneck during training.\\n\\nAdditionally, you\'ll need to install the following dependencies:\\n\\n```txt\\ndatasets\\ntorch\\ntransformers\\ntrl\\n```\\n\\n## Tuning Hyperparameters\\n\\n[BAAI/Infinity-Instruct](https://huggingface.co/datasets/BAAI/Infinity-Instruct) provides several officially fine-tuned models, including [Llama3.1-70B](https://huggingface.co/BAAI/Infinity-Instruct-7M-Gen-Llama3_1-70B), [mistral-7B](https://huggingface.co/BAAI/Infinity-Instruct-7M-Gen-mistral-7B), [Qwen2-7B](https://huggingface.co/BAAI/Infinity-Instruct-3M-0625-Qwen2-7B) and [Yi-1.5-9B](https://huggingface.co/BAAI/Infinity-Instruct-3M-0625-Yi-1.5-9B). They also generously share the training details for these models.\\n\\nFor this tutorial, we\'ll use the [hyperparameters for Qwen2-7B](https://huggingface.co/BAAI/Infinity-Instruct-3M-0625-Qwen2-7B#training-details) as a reference. Here\'s how these hyperparameters map to training arguments in `trl`:\\n\\n- epoch: `--num_train_epochs`\\n- lr: `--learning_rate`\\n- lr_warmup_steps: `--warmup_steps`\\n- lr_decay_style: `--lr_scheduler_type` (Set to `cosine_with_min_lr` along with `min_lr`. Available scheduler options can be found [here](https://huggingface.co/docs/transformers/main_classes/optimizer_schedules#transformers.SchedulerType).)\\n- min_lr: `--lr_scheduler_kwargs` (Set to `\\"{\\\\\\"min_lr\\\\\\": 0}\\"`. This argument isn\'t clearly documented; I discovered it through [this PR](https://github.com/huggingface/transformers/pull/29341) and [this test case](https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/tests/trainer/test_trainer.py#L1065).)\\n- weight_decay: `--weight_decay`\\n- adam_beta1: `--adam_beta1`\\n- adam_beta2: `--adam_beta2`\\n- clip_grad: `--max_grad_norm`\\n\\nOne additional parameter worth mentioning is the `global_batch_size`, which isn\'t directly set in the training script. The global batch size is determined by the equation:\\n\\n`global_batch_size = per_device_train_batch_size * gradient_accumulation_steps * numGPUs`.\\n\\nFor example, if our target global batch size is 528 and we\'re using 8 GPUs, the local batch size (per GPU) would be:\\n\\n`528 / 8 = 66`.\\n\\nIf we can fit 2 samples per batch in each GPU, we can then set `per_device_train_batch_size` to 2 and `gradient_accumulation_steps` to 33.\\n\\nAnother important consideration is training precision. Modern GPUs (Ampere series or newer) supports `bf16` and `tf32`, while older GPUs only support `fp16` and `fp32`. When fine-tuning, make sure the precision matches that of the base model. Specifically, avoid using `fp16` if the base model was trained with `bf16`. For more details, refer to [this PR](https://github.com/huggingface/transformers/pull/10956).\\n\\nYou can find the data type of your base model by the `torch_dtype` field in the `config.json` file. So if you\'re fine-tuning a `bf16` model but don\'t have access to Ampere or newer GPUs (like me), it\'s best to stick with `fp32` for now.\\n\\nNow that we\'ve covered the essential hyperparameters and considerations, let\'s move on to some optimization techniques that will help improve training efficiency and resource usage.\\n\\n### Gradient Accumulation\\n\\nYou may have noticed that I used `per_device_train_batch_size` and `gradient_accumulation_steps` to calculate the local batch size. Gradient accumulation allows you to accumulate gradients over multiple mini-batches before updating the model. This technique is particularly useful when the desired batch size exceeds your hardware\'s memory capacity.\\n\\nAs a general guideline:\\n\\n- Use the largest `per_device_train_batch_size` that fits within your VRAM\\n- Adjust `gradient_accumulation_steps` to achieve your target batch size if necessary.\\n\\nThis way, you can effectively simulate a larger batch size without running into memory limitations\\n\\n### Gradient Checkpointing\\n\\nGradient checkpointing is a memory optimization technique that reduces memory usage by trading off computation. During training, a large portion of memory is used to store intermediate activations for backpropagation. Gradient checkpointing reduces this memory usage by selectively saving a subset of activations and recomputing the rest during the backward pass.\\n\\nNote: According to https://pytorch.org/docs/stable/checkpoint.html:\\n\\n> There are currently two checkpointing implementations available, determined by the `use_reentrant` parameter. It is recommended that you use `use_reentrant=False`.\\n\\nYou can read that section for a deeper understanding of  the differences between the two implementations.\\n\\nAt the time of writing, the `transformers` library (v4.48.1) [uses the reentrant implementation by default](https://github.com/huggingface/transformers/blob/2e752ead46a8845e8a160d2043c1336447895690/src/transformers/modeling_utils.py#L2538). To use the non-reentrant version, you must explicitly pass the following argument:\\n\\n```sh\\n--gradient_checkpointing_kwargs \\"{\\\\\\"use_reentrant\\\\\\": false}\\"\\n```\\n\\n### ZeRO\\n\\nThere are several approaches to parallelizing training tasks, including **Data Parallelism (DP)**, **Tensor Parallelism (TP)**, **Pipeline Parallelism (PP)**, **Zero Redundancy Optimizer (ZeRO)**, **Sequence Parallelism** and **Expert Parallelism**. For a detailed overview of these methods, I recommend checking out [this excellent resource](https://github.com/stas00/ml-engineering/tree/master/training/model-parallelism#scalability-concepts).\\n\\nIn this tutorial, we\'ll focus on **ZeRO**, which provides greater efficiency than traditional DP without requiring modifications to the training code.\\n\\nZeRO (Zero Redundancy Optimizer) is a powerful technique for scaling training by reducing memory usage. If you\'re new to ZeRO, check out [the original paper](https://arxiv.org/abs/1910.02054) or [this detailed article](https://github.com/stas00/ml-engineering/tree/master/training/model-parallelism#zero-data-parallelism).\\n\\nZeRO has three stages, each targeting different aspects  of memory savings:\\n\\n- Stage 1: Reduces optimizer state memory.\\n- Stage 2: Further reduces gradient memory.\\n- Stage 3: Fully partitions model states, achieving the highest memory savings at the cost of increased communication overhead.\\n\\nStage 3 provides the greatest memory efficiency but can significantly slow down training if inter-GPU communication is not fast enough. As a general guideline:\\n\\n- Start with Stage 2.\\n- Try Stage 3 only if Stage 2 still leads to CUDA OOM.\\n\\nThat being said, it\u2019s always worth testing both Stage 2 and Stage 3 on your setup to determine which one performs better on your hardware.\\n\\nFor this tutorial, we will use the official implementation of ZeRO -- [DeepSpeed](https://www.deepspeed.ai/). To use DeepSpeed, you\'ll need to install it first. DeepSpeed provides C++/CUDA ops that can be pre-installed or JIT-compiled. If you choose the pre-installation option, refer to [this documentation](https://www.deepspeed.ai/tutorials/advanced-install/#pre-install-deepspeed-ops). we\u2019ll install DeepSpeed using the JIT method by running:\\n\\n```sh\\npip install deepspeed\\n```\\n\\nThe Hugging Face `transformers` library provides built-in support for DeepSpeed. You can enable it by specifying a DeepSpeed config file using the `--deepspeed` flag in your training script. For more information, refer to the [DeepSpeed documentation in transformers](https://huggingface.co/docs/transformers/deepspeed).\\n\\n### Liger Kernel\\n\\n[Liger Kernel](https://github.com/linkedin/Liger-Kernel) is a collection of Triton kernels developed by LinkedIn to reduce memory usage and increase training throughput. The best part is that it requires no complex configuration, making it an easy addition to your setup. To install it, run:\\n\\n```sh\\npip install liger-kernel\\n```\\n\\nOnce installed, add the `--use_liger` flag to your training script, and you\'ll automatically save VRAM without any extra setup or hassle. It\'s a straightforward way to optimize your training without sacrificing performance.\\n\\n### Sample Packing\\n\\nLarge models are trained on GPUs to leverage their parallelism. However, in the context of language models, where we train on text sequences, the length of each sample varies.\\n\\nThe traditional approach to handle variable-length sequences is to pad each sample to match the longest one in a batch. While this ensures uniform input dimensions, it also leads to considerable memory waste due to the padding.\\n\\n[Sample packing](https://arxiv.org/abs/2407.09105) addresses this issue by combining shorter samples into a single sequence. This technique allows for more efficient GPU memory usage, reducing waste and potentially speeding up training.\\n\\nWhile the concept is straightforward, Implementing it correctly can be one of the most challenging tasks of this experiment for me.\\n\\nAt the first glance, [trl supports packing dataset by simply passing an argument](https://huggingface.co/docs/trl/sft_trainer#packing-dataset--constantlengthdataset-). However, uppon [further investigation](https://github.com/huggingface/trl/blob/f34b70a32ef2820d3fd5c5b1ff6d1fd1e7799f04/trl/trainer/sft_trainer.py#L459), I found that the implementation might not suit my needs. As pointed out in [this issue](https://github.com/huggingface/trl/issues/805), the attention mask is not handled properly, which can lead to potential cross contamination in attention between sequences. The following image illustrates this issue clearly. On the left is the result of using `--packing`, and on the right is the correct way to pack samples:\\n\\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/63eb008e5c837d9968f1eb71/lzpKqOADV5mdOdclPbQ9C.png)\\n\\nAfter further digging, I found that, at least for now, [\'the correct way of packing\' is supported only with Flash Attention](https://github.com/huggingface/transformers/issues/27640#issuecomment-2619471784). If you don\'t have access to Ampere or newer GPUs, you may need to stick with the traditional padding approach.\\n\\nHowever, if you\'re lucky enough to have those GPUs, you can follow [this blog post](https://huggingface.co/blog/packing-with-FA2) to enable sample packing during training. Note that I haven\'t personally validated this approach. Also, as of writing, there are some PRs related to this feature that aren\'t released yet (for example [this one](https://github.com/huggingface/trl/pull/2158)). To access this functionality, you may need to install `trl` and `transformers` from source:\\n\\n```sh\\npip install git+https://github.com/huggingface/trl\\npip install git+https://github.com/huggingface/tranformers\\n```\\n\\n## Distributed Training\\n\\nWith all the optimizations in place, we\'re now ready to scale our SFT experiment across multiple GPUs. To do so, we can use tools like [torchrun](https://pytorch.org/docs/stable/elastic/run.html), [deepspeed](https://www.deepspeed.ai/getting-started/) or [accelerate](https://huggingface.co/docs/accelerate/index). Personally I prefer `torchrun` for its simplicity and ease of use.\\n\\nBy running the following command, we can distribute the training job across multiple GPUs:\\n\\nOh, and don\'t forget to set up `wandb` for logging \u2014 we\'re doing proper fine-tuning now! \ud83d\ude09\\n\\n<details>\\n    <summary>sft2.sh</summary>\\n\\n    ```sh\\n    torchrun \\\\\\n    --nproc_per_node 8 \\\\\\n    sft.py \\\\\\n    --model_name_or_path Qwen/Qwen2.5-3B \\\\\\n    --dataset_name BAAI/Infinity-Instruct \\\\\\n    --dataset_config 0625 \\\\\\n    --do_train \\\\\\n    --learning_rate 1e-5 \\\\\\n    --lr_scheduler_type cosine_with_min_lr \\\\\\n    --lr_scheduler_kwargs \\"{\\\\\\"min_lr\\\\\\": 0}\\" \\\\\\n    --warmup_steps 40 \\\\\\n    --weight_decay 0.0 \\\\\\n    --max_grad_norm 1.0 \\\\\\n    --adam_beta1 0.9 \\\\\\n    --adam_beta2 0.95 \\\\\\n    --per_device_train_batch_size 11 \\\\\\n    --gradient_accumulation_steps 6 \\\\\\n    --gradient_checkpointing \\\\\\n    --gradient_checkpointing_kwargs \\"{\\\\\\"use_reentrant\\\\\\": false}\\" \\\\\\n    --num_train_epochs 3 \\\\\\n    --use_liger \\\\\\n    --deepspeed ./ds-config.json \\\\\\n    --output_dir /tmp/Qwen2.5-3B-Infinity-Instruct-0625 \\\\\\n    --report_to wandb \\\\\\n    --run_name my-second-sft-exp\\n    ```\\n\\n</details>\\n\\n<details>\\n    <summary>ds-config.json</summary>\\n\\n    ```json\\n    {\\n        \\"fp16\\": {\\n            \\"enabled\\": false\\n        },\\n        \\"optimizer\\": {\\n            \\"type\\": \\"AdamW\\",\\n            \\"params\\": {\\n                \\"lr\\": \\"auto\\",\\n                \\"betas\\": \\"auto\\",\\n                \\"eps\\": \\"auto\\",\\n                \\"weight_decay\\": \\"auto\\"\\n            }\\n        },\\n        \\"zero_optimization\\": {\\n            \\"stage\\": 2,\\n            \\"overlap_comm\\": false,\\n            \\"allgather_bucket_size\\": 5e8,\\n            \\"reduce_bucket_size\\": \\"auto\\",\\n            \\"allgather_partitions\\": true,\\n            \\"reduce_scatter\\": true,\\n            \\"contiguous_gradients\\": true,\\n            \\"round_robin_gradients\\": true\\n        },\\n        \\"gradient_accumulation_steps\\": \\"auto\\",\\n        \\"gradient_clipping\\": \\"auto\\",\\n        \\"train_batch_size\\": \\"auto\\",\\n        \\"train_micro_batch_size_per_gpu\\": \\"auto\\",\\n        \\"wall_clock_breakdown\\": false\\n    }\\n    ```\\n\\n</details>\\n\\nThanks to all the optimizations, I was able to fine-tune a 3B model instead of the 0.5B model used in the first part.\\n\\nIt did take a considerable amount of time (about 133 hours) to complete the training on V100s, so I highly recommend use modern GPUs and enabling Flash Attention and sample packing for better performance.\\n\\n## Evaluating\\n\\nNow that the training is complete, it\u2019s important to evaluate whether everything was done correctly.\\n\\nA quick way to check the model\'s performance is to interact with it. You can refer to [the quickstart section of the official SFT model](https://huggingface.co/Qwen/Qwen2.5-3B-Instruct#quickstart) to try it out. Here\'s an example of interacting with the model I just fine-tuned:\\n\\n```text\\nMe: Give me a short introduction to large language model.\\n\\nAI: A large language model (LLM) is a type of artificial intelligence (AI) that is designed to understand and generate human language. These models are trained on vast amounts of text data, allowing them to learn patterns, structures, and nuances\\n```\\n\\n(Note: that the response from the AI is truncated due to the `max_new_tokens` I set, but you can see that the model is responding appropriately.)\\n\\nWhile direct interactions are useful for quick checks, formal evaluations are essential for more rigorous validation. Evaluating LLMs is quite a broad topic, and I\'m only going to share a few tips here.\\n\\nThere are several frameworks available for evaluating LLMs, making it challenging to choose the best one, and comparing results from different frameworks can sometimes lead to unfair conclusions.\\n\\nOne well-known evaluation platform is the [Open LLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/), which ranks LLMs based on their evaluation results. The Open LLM Leaderboard uses [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) as its backend. By using this same tool, you can ensure fair comparisons with models in the leaderboard. So for this tutorial, I\'ll to use `lm-evaluation-harness` to run the same evaluations used on the Open LLM Leaderboard to assess the model I just fine-tuned.\\n\\nThe `lm-evaluation-harness` [integrates all the tasks used in the Open LLM Leaderboard](https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/tasks/leaderboard/README.md). To evaluate your model on these tasks, you can run the following command:\\n\\n```sh\\nlm_eval \\\\\\n  --model hf \\\\\\n  --model_args pretrained=$MODEL_YOU_WANT_TO_EVAL \\\\\\n  --tasks leaderboard\\n```\\n\\n### MATH-hard Task Unavailable\\n\\nHowever, As of writing, the `competition_math` dataset is [currently unavailable due to legal issues](https://huggingface.co/datasets/hendrycks/competition_math/discussions/5). As a result, we\'ll need to skip the `MATH-hard` task that relies on this dataset. You can modify your script to include all other tasks except `leaderboard_math_hard`:\\n\\n```sh\\nlm_eval \\\\\\n  --model hf \\\\\\n  --model_args pretrained=$MODEL_YOU_WANT_TO_EVAL \\\\\\n  --tasks leaderboard_bbh,leaderboard_gpqa,leaderboard_ifeval,leaderboard_mmlu_pro,leaderboard_musr\\n```\\n\\n### Evaluating Code Generation\\n\\nIn addition to the leaderboard evaluations, if you\'re interested in evaluating your model on code generation tasks (such as [humaneval](https://github.com/EleutherAI/lm-evaluation-harness/tree/main/lm_eval/tasks/humaneval)), keep in mind that the generated code usually needs to be executed to evaluate its correctness. Since executing LLM generated code can be risky, most frameworks will default to abort on such tasks. To allow code execution during evaluation, you need to set `HF_ALLOW_CODE_EVAL` to `1` and include the `--confirm_run_unsafe_code` argument in your evaluation command:\\n\\n```sh\\nlm_eval \\\\\\n  --model hf \\\\\\n  --model_args pretrained=$MODEL_YOU_WANT_TO_EVAL \\\\\\n  --tasks leaderboard_bbh,leaderboard_gpqa,leaderboard_ifeval,leaderboard_mmlu_pro,leaderboard_musr \\\\\\n  --confirm_run_unsafe_code  # Add this line\\n```\\n\\n## Conclusion\\n\\nIn this post, we\u2019ve covered everything from the basic setup to advanced techniques for scaling large language models in a single-node, multi-GPU environment. By utilizing DeepSpeed and trl, we can efficiently fine-tune models like Qwen2-3B and beyond, even on hardware that would otherwise be unable to support such models. I\'ve also uploaded the fine-tuned model to the Hugging Face model hub, so you can try it out for yourself: https://huggingface.co/jlzhou/Qwen2.5-3B-Infinity-Instruct-0625.\\n\\nIn the next part of this series, we\u2019ll explore distributed training across multiple nodes, tackling more complex setups with multiple GPUs across different machines. Stay tuned!"},{"id":"distributed-sft-part-1","metadata":{"permalink":"/distributed-sft-part-1","editUrl":"https://github.com/edwardzjl/edwardzjl.github.io/blob/main/blog/2025-01-23-distributed-sft-part-1/index.md","source":"@site/blog/2025-01-23-distributed-sft-part-1/index.md","title":"Distributed SFT Part 1: Starting Locally","description":"Introduction","date":"2025-01-23T00:00:00.000Z","tags":[{"inline":true,"label":"LLM","permalink":"/tags/llm"},{"inline":true,"label":"distributed-training","permalink":"/tags/distributed-training"}],"readingTime":7.93,"hasTruncateMarker":true,"authors":[{"name":"Junlin Zhou","title":"Fullstack Engineer @ ZJU ICI","url":"https://github.com/edwardzjl","imageURL":"https://github.com/edwardzjl.png","key":"jlzhou","page":null}],"frontMatter":{"slug":"distributed-sft-part-1","authors":["jlzhou"],"tags":["LLM","distributed-training"]},"unlisted":false,"prevItem":{"title":"Distributed SFT Part 2: Scaling Locally","permalink":"/distributed-sft-part-2"},"nextItem":{"title":"[\u8bd1] JSON\u683c\u5f0f\u4f5c\u4e3a\u914d\u7f6e\u6587\u4ef6\u7684\u7f3a\u70b9","permalink":"/the-downsides-of-json-for-config-files"}},"content":"## Introduction\\n\\nWelcome to this series of articles documenting the lessons I learned during my first attempt at running distributed supervised fine-tuning (SFT) tasks using [trl](https://github.com/huggingface/trl) and [DeepSpeed](https://github.com/microsoft/DeepSpeed).\\n\\nThis series will walk you through my journey, starting with a simple local experiment and progressively scaling up to a distributed environment. The three parts of this series are:\\n\\n- **Part 1: The Local Experiment** -- I will show you how I ran my very first local SFT experiment, following the official [trl documentation](https://huggingface.co/docs/trl/sft_trainer).\\n\\n- **Part 2: Multi GPU** -- We will leverage **single-machine, multi-GPU** parallel training to complete a full SFT task in our local environment.\\n\\n- **Part 3: Multi Machine** -- We\'ll take things a step further by submitting the same training task to a Kubernetes cluster, utilizing **multi-machine, multi-GPU** training with [Kubeflow\'s Training Operator](https://github.com/kubeflow/training-operator).\\n\\nA quick note about myself: I\'m a software development engineer who is fairly new to the field of deep learning. If these articles seem too basic for you, I appreciate your patience as I navigate this learning journey.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Prerequisites\\n\\nTo follow this tutorial, you\'ll need a machine with at least one NVIDIA GPU. I ran the experiment on a V100 without encountering any memory issues. If your GPU has less than 32GB of VRAM, you may need to reduce the `per_device_train_batch_size` or consider using truncation (although this is not recommended) to prevent CUDA out-of-memory (OOM) errors.\\n\\nYou\'ll also need the following dependencies:\\n\\n```txt\\ndatasets\\ntransformers\\ntrl\\ntorch\\n```\\n\\n## Training\\n\\nThe `trl` library offers some excellent example training scripts, and we\'ll start with this one: https://github.com/huggingface/trl/blob/main/trl/scripts/sft.py\\n\\nCopy the script to your development machine (or notebook), select a base model, and pick an SFT dataset to run the experiment. For this experiment, I chose [Qwen/Qwen2.5-0.5B](https://huggingface.co/Qwen/Qwen2.5-0.5B) as the base model for its compact size, and [BAAI/Infinity-Instruct](https://huggingface.co/datasets/BAAI/Infinity-Instruct) as the SFT dataset (somehow randomly \ud83d\ude0c). You can explore other interesting datasets here: https://github.com/mlabonne/llm-datasets.\\n\\n### Command-line Arguments\\n\\nThe training script (`sft.py`) exposes a variety of useful command-line arguments that allow you to customize the fine-tuning process. These arguments are mapped to specific properties in the following classes:\\n\\n- [ScriptArguments](https://huggingface.co/docs/trl/v0.13.0/en/script_utils#trl.ScriptArguments)\\n- [ModelConfig](https://github.com/huggingface/trl/blob/v0.13.0/trl/trainer/model_config.py#L20)\\n- [SFTConfig](https://huggingface.co/docs/trl/v0.13.0/en/sft_trainer#trl.SFTConfig), which extends [TrainingArguments](https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments)\\n\\nYou can pass any of these arguments directly from the command line by prepending them with `--`. For instance, passing `--dataset_name` will set the `dataset_name` field in the `trl.ScriptArguments` class.\\n\\nLet\'s take a look at the arguments used for this tutorial:\\n\\n- `--model_name_or_path`: Specifies the base model to fine-tune.\\n- `--dataset_name`: Defines the dataset to use for fine-tuning.\\n- `--dataset_config`: Some datasets come with multiple configurations (versions). This argument lets you choose the version you want to use.\\n- `--do_train`: Tells the script to start the training process.\\n- `--per_device_train_batch_size`: Defines the batch size for each GPU.\\n- `--output_dir`: Specifies the directory where the model will be saved.\\n- `--max_steps`: Sets the maximum number of training steps.\\n- `--logging_steps`: Sets how often logs are recorded during training.\\n\\nFor convenience, I prefer to save the full command in a shell script for easy execution. Here\'s the script I used for this tutorial:\\n\\n```sh\\npython sft.py \\\\\\n  --model_name_or_path Qwen/Qwen2.5-0.5B \\\\\\n  --dataset_name BAAI/Infinity-Instruct \\\\\\n  --dataset_config 0625 \\\\\\n  --do_train \\\\\\n  --per_device_train_batch_size 4 \\\\\\n  --output_dir /tmp/my-first-sft-exp \\\\\\n  --max_steps 10 \\\\\\n  --logging_steps 1\\n```\\n\\nNotes:\\n\\n- I selected the smallest version of the dataset and limited the experiment to just 10 steps for a quicker run.\\n- Since the training is only 10 steps, I set `--logging_steps` to 1 to see logs more frequently.\\n- The `--per_device_train_batch_size` is set to 4, as the goal here isn\'t model quality but simply to run the experiment without CUDA OOM. Any number that can fit in your VRAM should work.\\n\\n> *Updated 2025-02-18:*\\n>\\n> `trl` provides a convenient helper function to parse training args from a YAML file, you can find more details [here](https://huggingface.co/docs/trl/main/script_utils#trl.TrlParser.parse_args_and_config).\\n>\\n> With this feature, you can save the above training arguments in a YAML file (e.g., `recipe.yaml`) as follows:\\n>\\n> ```yaml\\n> model_name_or_path: Qwen/Qwen2.5-0.5B\\n> dataset_name: BAAI/Infinity-Instruct\\n> dataset_config: \'0625\'\\n> do_train: true\\n> per_device_train_batch_size: 4\\n> output_dir: /tmp/my-first-sft-exp\\n> max_steps: 10\\n> logging_steps: 1\\n> ```\\n>\\n> And launch the training with:\\n> ```sh\\n> python sft.py --config recipe.yaml\\n> ```\\n\\n### The Oops\\n\\nNow if you use the same dataset and execute the same script, you\'ll likely encounter a (not so helpful) error message:\\n\\n```console\\n$ ./quickstart.sh \\nResolving data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 35/35 [00:00<00:00, 50.35it/s]\\nMap:   0%|                                                                                                                                                                         | 0/659808 [00:00<?, ? examples/s]\\nTraceback (most recent call last):\\n  File \\"/home/jovyan/sft-walkthrough/sft.py\\", line 126, in <module>\\n    main(script_args, training_args, model_args)\\n  File \\"/home/jovyan/sft-walkthrough/sft.py\\", line 97, in main\\n    trainer = SFTTrainer(\\n  ...\\n  File \\"/home/jovyan/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py\\", line 416, in tokenize\\n    element[dataset_text_field] if formatting_func is None else formatting_func(element),\\n  File \\"/home/jovyan/.local/lib/python3.10/site-packages/datasets/formatting/formatting.py\\", line 277, in __getitem__\\n    value = self.data[key]\\nKeyError: \'text\'\\n```\\n\\n### The Fix\\n\\n> *Updated 2025-02-18:*\\n>\\n> - Starting from trl 0.15.0 (in [this PR](https://github.com/huggingface/trl/pull/2405)) the \'conversations\' column is no longer supported. We need to rename it to \'messages\'.\\n> - In [this PR](https://github.com/huggingface/trl/pull/2862) (not yet released as of writing), support for the \'conversations\' column is back and the whole preprocessing is simplified, we no longer need to map the dict keys(\'from\' -> \'role\', \'value\' -> \'content\') ourselves.\\n>\\n> *Updated 2025-02-19:*\\n>\\n> The above PR is released in trl 0.15.1.\\n\\nThis error message is a bit confusing--it states that the `SFTTrainer` requires the dataset to have a \'text\' field. However, according to the [dataset format and types](https://huggingface.co/docs/trl/dataset_formats#overview-of-the-dataset-formats-and-types), \'text\' is used for standard dataset, while \'messages\' should be used for conversational datasets. After a lot of googling, I came across [this tracking issue](https://github.com/huggingface/trl/issues/2071), [this line of code](https://github.com/huggingface/trl/blob/v0.13.0/trl/trainer/sft_trainer.py#L250) and [this function](https://github.com/huggingface/trl/blob/v0.13.0/trl/extras/dataset_formatting.py#L78). It seems that for the current implementation (`trl == 0.13.0`) we have two options:\\n\\n1. Format the dataset ourselves (apply a chat template) and place the formatted data into the \'text\' field.\\n2. Convert our dataset in a way that allows `trl` to handle the transformation for us.\\n\\nFor the second option to work, the dataset must:\\n\\n- Contain a \'messages\' or \'conversations\' field.\\n- Have each element in the \'messages\' (or \'conversations\') field include both a \'content\' field and a \'role\' field.\\n\\nExamining the dataset I used revealed a mismatch: while it has a \'conversations\' field, the elements inside use \'from\' and \'value\' as keys instead of \'role\' and \'content\'. As a lazy coder, I opted for the second approach and updated the training script accordingly. Additionally, I also remove all other fields in the dataset, as they are unused for the SFT task. Removing them will slightly reduce memory footprint and speed up processing.\\n\\n```python\\n...\\ndef main(script_args, training_args, model_args):\\n    ...\\n    ################\\n    # Dataset\\n    ################\\n    dataset = load_dataset(script_args.dataset_name, name=script_args.dataset_config)\\n\\n    def convert_fields(message: dict) -> dict:\\n        _message = {\\n          \\"role\\": message[\\"from\\"],\\n          \\"content\\": message[\\"value\\"],\\n        }\\n        # Qwen2.5 tokenizer only supports \\"system\\", \\"user\\", \\"assistant\\" and \\"tool\\"\\n        # See <https://huggingface.co/Qwen/Qwen2.5-3B/blob/main/tokenizer_config.json#L198>\\n        if _message[\\"role\\"] == \\"human\\":\\n            _message[\\"role\\"] = \\"user\\"\\n        elif _message[\\"role\\"] == \\"gpt\\":\\n            _message[\\"role\\"] = \\"assistant\\"\\n        elif _message[\\"role\\"] == \\"system\\":\\n            # nothing to be done.\\n            ...\\n        else:\\n            # In case there are any other roles, print them so we can improve in next iteration.\\n            print(_message[\\"role\\"])\\n        return _message\\n\\n    def convert_messages(example):\\n        example[\\"conversations\\"] = [convert_fields(message) for message in example[\\"conversations\\"]]\\n        return example\\n\\n    # remove unused fields\\n    dataset = dataset.remove_columns([\\"id\\", \\"label\\", \\"langdetect\\", \\"source\\"]).map(convert_messages)\\n    ...\\n```\\n\\nWith that update, the script ran without any issues! You should be able to see the training log like:\\n\\n```console\\n$ ./quickstart.sh \\nResolving data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 35/35 [00:02<00:00, 17.26it/s]\\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 659808/659808 [01:19<00:00, 8280.44 examples/s]\\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 659808/659808 [08:33<00:00, 1284.45 examples/s]\\n{\'loss\': 1.8859, \'grad_norm\': 14.986075401306152, \'learning_rate\': 1.8e-05, \'epoch\': 0.0}                                                                                                                                     \\n{\'loss\': 1.4527, \'grad_norm\': 13.9092378616333, \'learning_rate\': 1.6000000000000003e-05, \'epoch\': 0.0}                                                                                                                        \\n{\'loss\': 1.467, \'grad_norm\': 7.388503074645996, \'learning_rate\': 1.4e-05, \'epoch\': 0.0}                                                                                                                                       \\n{\'loss\': 1.7757, \'grad_norm\': 9.457520484924316, \'learning_rate\': 1.2e-05, \'epoch\': 0.0}                                                                                                                                      \\n{\'loss\': 1.9043, \'grad_norm\': 10.256357192993164, \'learning_rate\': 1e-05, \'epoch\': 0.0}                                                                                                                                       \\n{\'loss\': 1.6163, \'grad_norm\': 10.774249076843262, \'learning_rate\': 8.000000000000001e-06, \'epoch\': 0.0}                                                                                                                       \\n{\'loss\': 1.1774, \'grad_norm\': 5.897563457489014, \'learning_rate\': 6e-06, \'epoch\': 0.0}                                                                                                                                        \\n{\'loss\': 1.8093, \'grad_norm\': 8.3130464553833, \'learning_rate\': 4.000000000000001e-06, \'epoch\': 0.0}                                                                                                                          \\n{\'loss\': 1.8387, \'grad_norm\': 7.102719306945801, \'learning_rate\': 2.0000000000000003e-06, \'epoch\': 0.0}                                                                                                                       \\n{\'loss\': 1.4251, \'grad_norm\': 9.853829383850098, \'learning_rate\': 0.0, \'epoch\': 0.0}                                                                                                                                          \\n{\'train_runtime\': 38.8598, \'train_samples_per_second\': 1.029, \'train_steps_per_second\': 0.257, \'train_loss\': 1.635251808166504, \'epoch\': 0.0}                                                                                 \\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:38<00:00,  3.89s/it]\\n```\\n\\n## Conclusion\\n\\nIn this first part, we\'ve walked through setting up a local SFT experiment using `trl`. This library provides a user-friendly interface for fine-tuning LLMs with custom datasets. We also covered the correct dataset format required for `trl`\'s `SFTTrainer` and how to preprocess datasets to meet these requirements.\\n\\nIn the next part, we\'ll delve into scaling this setup locally using a single-node, multi-GPU configuration to tackle a complete SFT task. Additionally, we\'ll explore various optimization techniques to fit a bigger model into your GPU and accelerate the training process. Stay tuned!"},{"id":"the-downsides-of-json-for-config-files","metadata":{"permalink":"/the-downsides-of-json-for-config-files","editUrl":"https://github.com/edwardzjl/edwardzjl.github.io/blob/main/blog/2019-08-09-the-downsides-of-json-for-config-files/index.md","source":"@site/blog/2019-08-09-the-downsides-of-json-for-config-files/index.md","title":"[\u8bd1] JSON\u683c\u5f0f\u4f5c\u4e3a\u914d\u7f6e\u6587\u4ef6\u7684\u7f3a\u70b9","description":"\u7ffb\u8bd1\u81ea\u8fd9\u7bc7\u6587\u7ae0","date":"2019-08-09T00:00:00.000Z","tags":[{"inline":true,"label":"json","permalink":"/tags/json"}],"readingTime":4.89,"hasTruncateMarker":true,"authors":[{"name":"Junlin Zhou","title":"Fullstack Engineer @ ZJU ICI","url":"https://github.com/edwardzjl","imageURL":"https://github.com/edwardzjl.png","key":"jlzhou","page":null}],"frontMatter":{"slug":"the-downsides-of-json-for-config-files","authors":["jlzhou"],"tags":["json"]},"unlisted":false,"prevItem":{"title":"Distributed SFT Part 1: Starting Locally","permalink":"/distributed-sft-part-1"},"nextItem":{"title":"\u7cfb\u7edf\u4e2d\u72b6\u6001\u4e3a static \u7684\u670d\u52a1","permalink":"/static-service"}},"content":"> \u7ffb\u8bd1\u81ea[\u8fd9\u7bc7\u6587\u7ae0][1]\\n\\n\u6211\u6700\u8fd1\u63a5\u89e6\u5230\u8bb8\u591a\u9879\u76ee\u5c06 `JSON` \u7528\u4f5c\u914d\u7f6e\u6587\u4ef6\u3002\u6211\u8ba4\u4e3a\u8fd9\u4e0d\u662f\u4e00\u4e2a\u597d\u4e3b\u610f\u3002\\n\\n`JSON` \u4ece\u8bbe\u8ba1\u4e4b\u521d\u5c31\u4e0d\u662f\u7528\u4e8e\u505a\u914d\u7f6e\u6587\u4ef6\u7684\uff0c\u8fd9\u4e5f\u4e0d\u662f\u5b83\u64c5\u957f\u7684\u9886\u57df\u3002`JSON` \u7684\u76ee\u6807\u662f \\"\u8f7b\u91cf\u7ea7\u6570\u636e\u4ea4\u6362\u683c\u5f0f\\", \u540c\u65f6\u5177\u6709 \\"\u6613\u4e8e\u4eba\u7c7b\u8bfb\u5199\\", \\"\u6613\u4e8e\u4ee3\u7801\u89e3\u6790\u548c\u751f\u6210\\" \u7684\u7279\u70b9\u3002\u5b83\u5728\u5bf9 \\"\u4eba\u7c7b\u800c\u8a00\u7684\u4fbf\u5229\u6027\\" \u548c \\"\u5bf9\u673a\u5668\u800c\u8a00\u7684\u4fbf\u5229\u6027\\" \u4e4b\u95f4\u53d6\u5f97\u4e86\u8f83\u597d\u7684\u5e73\u8861, \u5728\u8bb8\u591a\u5e94\u7528\u573a\u666f\u4e0b\u90fd\u662f\u6bd4 `XML` \u66f4\u597d\u7684\u66ff\u4ee3\u65b9\u6848\u3002\\n\\n\u7136\u800c\uff0c\u5c06 `JSON` \u7528\u4e8e\u5176\u4ed6\u76ee\u7684\u6709\u70b9\u7c7b\u4f3c\u4e8e\u8bf4 \\"\u563f\uff0c\u8fd9\u628a\u9524\u5b50\u975e\u5e38\u9002\u5408\u9489\u9489\u5b50\uff01\u6211\u559c\u6b22\u5b83\uff01\u4e3a\u4ec0\u4e48\u4e0d\u7528\u5b83\u6765\u62e7\u87ba\u4e1d\uff01\\" \u5f53\u7136\u5b83\u4e0d\u662f\u5b8c\u5168\u4e0d\u80fd\u7528\uff0c\u53ea\u662f\u4e0d\u5408\u9002\u505a\u8fd9\u6837\u7684\u5de5\u4f5c\u3002\\n\\n\x3c!-- truncate --\x3e\\n\\n\u76ee\u524d\u4e3a\u6b62\uff0c\u5c06 `JSON` \u7528\u4f5c\u5176\u5b83\u7528\u9014\u6700\u5927\u7684\u95ee\u9898\u5728\u4e8e\u4e0d\u80fd\u5728 `JSON` \u6587\u4ef6\u4e2d\u6dfb\u52a0\u6ce8\u91ca\u3002\u67d0\u4e9b\u7279\u5b9a\u7684 `JSON` \u89e3\u6790\u5668\u652f\u6301\u5728 `JSON` \u4e2d\u6dfb\u52a0\u6ce8\u91ca\uff0c\u4f46\u662f\u7edd\u5927\u90e8\u5206\u7684\u89e3\u6790\u5668\u90fd\u4e0d\u652f\u6301\u3002`JSON` \u7684\u53d1\u660e\u8005 `Douglas Crockford` \u58f0\u79f0 `JSON` \u6700\u5f00\u59cb\u662f\u652f\u6301\u6ce8\u91ca\u7684\uff0c\u7136\u800c\u7531\u4e8e\u4e00\u4e9b\u539f\u56e0\uff0c\u4ed6\u7279\u610f\u79fb\u9664\u4e86\u5bf9\u6ce8\u91ca\u7684\u652f\u6301\u3002\u60f3\u8981\u6df1\u5165\u7814\u7a76\u7684\u670b\u53cb\u53ef\u4ee5\u770b[\u8fd9\u91cc][2]\u3002\\n\\n\u6211\u4eec\u5728\u5199\u914d\u7f6e\u6587\u4ef6\u65f6\u7ecf\u5e38\u4f1a\u9047\u5230\u9700\u8981\u6dfb\u52a0\u6ce8\u91ca\u7684\u573a\u666f\u3002\u4f8b\u5982\u89e3\u91ca\u4e3a\u4ec0\u4e48\u5c06\u914d\u7f6e\u9879\u8bbe\u7f6e\u4e3a\u5f53\u524d\u7684\u503c\uff0c\u6dfb\u52a0\u4e00\u4e9b\u52a9\u8bb0\u7b26\u6216\u662f\u6ce8\u610f\u4e8b\u9879\uff0c\u5bf9\u4e8e\u9519\u8bef\u914d\u7f6e\u7684\u8b66\u544a\uff0c\u5728\u6587\u4ef6\u4e2d\u4fdd\u5b58\u4e00\u4efd\u57fa\u7840\u7684 `changelog`\uff0c\u53c8\u6216\u5355\u7eaf\u662f\u5728debug\u65f6\u9700\u8981\u6ce8\u91ca\u6389\u4e00\u4e9b\u914d\u7f6e\u9879\u3002\\n\\n\u4e00\u4e2a\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6cd5\u662f\u5c06\u539f\u672c\u7684\u6570\u636e\u5b58\u50a8\u5728\u4e00\u4e2a object \u4e2d\uff0c\u5728\u8fd9\u4e2a object \u4e2d\u901a\u8fc7\u4e24\u4e2a\u6761\u76ee\u5206\u522b\u5b58\u50a8\u6570\u636e\u548c\u6ce8\u91ca\u3002\u4f8b\u5982\u539f\u672c\u7684\u914d\u7f6e\u6587\u4ef6\u5982\u4e0b\uff1a\\n\\n```json\\n{\\n  \\"config_name\\": \\"config_value\\"\\n}\\n```\\n\\n\u4fee\u6539\u540e\u53d8\u6210\u5982\u4e0b\u5f62\u5f0f:\\n\\n```json\\n{\\n  \\"config_name\\": {\\n\\t  \\"actual_data\\": \\"config_value\\",\\n\\t\\t\\"comment\\": \\"a comment\\"\\n  }\\n}\\n```\\n\\n\u4f46\u662f\u5728\u6211\u770b\u6765\u8fd9\u79cd\u65b9\u5f0f\u4e11\u7684\u538b\u6279\u3002\\n\\n\u8fd8\u6709\u4e00\u4e9b\u4eba\u6307\u51fa\u53ef\u4ee5\u901a\u8fc7 commit log \u7684\u5f62\u5f0f\u6765\u5b9e\u73b0\u6ce8\u91ca *\uff08\u8bd1\u8005\uff1a\u4e0d\u6e05\u695a\u4ed6\u8fd9\u91cc\u6307\u7684\u662f\u4e0d\u662f git commit log\uff0c\u5982\u679c\u662f\u7684\u8bdd\u628a\u8fd9\u4e2a\u5f53\u4f5c\u6ce8\u91ca\u65b9\u5f0f\u597d\u50cf\u5341\u5206\u96be\u7528\u5427\uff1f\uff09*\uff0c\u4f46\u662f\u53c8\u6709\u51e0\u4e2a\u4eba\u4f1a\u53bb\u7ec6\u8bfb commit history\uff1f\\n\\n\u4e00\u4e9b\u57fa\u4e8e `JSON` \u8fdb\u884c\u6269\u5c55\u7684\u683c\u5f0f\uff0c\u4f8b\u5982 `JSON5`\uff0c`Hjson` \u548c `HOCON`\uff0c\u4ee5\u53ca\u4e00\u5c0f\u90e8\u5206 `JSON` \u89e3\u6790\u5668\u6dfb\u52a0\u4e86\u5bf9\u6ce8\u91ca\u7684\u652f\u6301\u3002\u8fd9\u5f88\u5b9e\u7528\uff0c\u4f46\u8fd9\u4e9b\u90fd\u5c5e\u4e8e `JSON` \u7684\u53d8\u79cd\uff0c\u56e0\u6b64\u4e0d\u5728\u672c\u7bc7\u7684\u8ba8\u8bba\u8303\u56f4\u4e4b\u5185\u3002\\n\\n\u540c\u65f6\u6211\u4e5f\u53d1\u73b0\u624b\u5de5\u7f16\u8f91 `JSON` \u7684\u7528\u6237\u4f53\u9a8c\u4e0d\u662f\u90a3\u4e48\u53cb\u597d\uff1a\u4f60\u5f97\u7559\u610f\u884c\u5c3e\u662f\u5426\u8981\u6dfb\u52a0\u9017\u53f7\uff0c\u5f97\u4e86\u89e3\u7528\u4e0d\u7528\u5f15\u53f7\u5bf9\u542b\u4e49\u7684\u5f71\u54cd\uff0c\u540c\u65f6 `JSON` \u4e5f\u4e0d\u652f\u6301\u5b57\u7b26\u4e32\u5185\u6362\u884c\u3002\u8fd9\u4e9b\u7279\u6027\u5bf9\u4e8e \\"\u8f7b\u91cf\u7ea7\u6570\u636e\u4ea4\u6362\u683c\u5f0f\\" \u800c\u8a00\u4e0d\u662f\u574f\u4e8b\uff0c\u4f46\u662f\u5bf9\u4e8e\u7f16\u8f91\u914d\u7f6e\u6587\u4ef6\u8fd9\u4ef6\u4e8b\u6765\u8bf4\u5374\u4e0d\u662f\u90a3\u4e48\u53ef\u7231\u3002\u603b\u7684\u6765\u8bf4\uff0c\u5c06 `JSON` \u7528\u4f5c\u914d\u7f6e\u6587\u4ef6\u867d\u7136\u53ef\u884c\uff0c\u4f46\u5e76\u4e0d\u4f18\u96c5\u3002\\n\\nMediaWiki \u7684\u65b0\u6269\u5c55\u7cfb\u7edf\u4fc3\u4f7f\u6211\u5199\u4e0b\u8fd9\u7bc7\u6587\u7ae0\u3002\u65e7\u7684\u7cfb\u7edf\u901a\u8fc7 PHP \u6587\u4ef6\u6765\u6302\u63a5\u6838\u5fc3\u4ee3\u7801\uff0c\u52a0\u8f7d\u6240\u9700\u7684\u4f9d\u8d56\u9879\u7b49\u3002\u65b0\u7cfb\u7edf\u901a\u8fc7 JSON \u6587\u4ef6\u5b9e\u73b0\u8fd9\u4e9b\u914d\u7f6e\u3002\u8fd9\u6837\u7684\u66f4\u65b0\u635f\u5931\u4e86 PHP \u90a3\u79cd\u80fd\u591f\u5de7\u5999\u89e3\u51b3\u4e0e\u5176\u4ed6\u63d2\u4ef6\u517c\u5bb9\u6027\u7684\u80fd\u529b\u3002 *\uff08\u8fd9\u6bb5\u6ca1\u770b\u61c2\uff09*\\n\\n\u540c\u65f6\u5b83\u4e5f\u5e26\u6765\u4e86\u66f4\u591a\u5b9e\u73b0\u590d\u6742\u5ea6\u3002\u65e7\u7684\u7cfb\u7edf\u5728\u5f15\u5165\u914d\u7f6e\u6587\u4ef6\u65f6\u4ec5\u4ec5\u9700\u8981\u4e00\u884c\u4ee3\u7801\uff1a\\n\\n```javascript\\nrequire(\'plugin/foo/plugin.php\');\\n```\\n\\n\u800c\u65b0\u7cfb\u7edf\u5374\u9700\u8981\u5bf9 JSON \u6587\u4ef6\u7684\u5185\u5bb9\u8fdb\u884c\u89e3\u6790\u3002\u8fd9\u5728\u63d0\u5347\u5b9e\u73b0\u590d\u6742\u5ea6\u7684\u540c\u65f6\uff0c\u4e5f\u63d0\u9ad8\u4e86 debug \u7684\u96be\u5ea6\u3002\\n*\uff08\u8fd9\u6bb5\u4e0d\u592a\u8d5e\u540c\uff0cXML \u4f5c\u4e3a\u914d\u7f6e\u6587\u4ef6\uff0c\u540c\u6837\u8981\u8fdb\u884c\u89e3\u6790\uff0c\u8fd9\u4e0d\u662f JSON \u7684\u95ee\u9898\u3002\uff09*\\n\\n\u4f7f\u7528 JSON \u6587\u4ef6\u5b58\u50a8\u57fa\u672c\u5143\u6570\u636e\u662f\u53ef\u884c\u7684\uff08\u66f4\u5bb9\u6613\u89e3\u6790\u4ee5\u53ca\u5728\u7f51\u7ad9\u4e0a\u663e\u793a\uff09\uff0c\u4f46\u4f7f\u7528\u5b83\u6765\u63cf\u8ff0\u4ee3\u7801\u7684\u5de5\u4f5c\u65b9\u5f0f\u5bf9\u6211\u6765\u8bf4\u662f\u6ee5\u7528 DC\uff08Declarative configuration \uff0c\u58f0\u660e\u6027\u914d\u7f6e\uff09\u3002\u6bd5\u7adf\uff0c\u8fd9\u662f\u4ee3\u7801\u7684\u5de5\u4f5c\u3002\\n\\n\u8bb8\u591a\u4eba\u95ee\u6211\u90a3\u5230\u5e95\u8be5\u7528\u4ec0\u4e48(\u6765\u505a\u914d\u7f6e\u6587\u4ef6)\uff0c\u8fd9\u5176\u5b9e\u662f\u4e2a\u5f88\u590d\u6742\u7684\u95ee\u9898\uff0c\u5173\u7cfb\u5230\u4f60\u7a0b\u5e8f\u7684\u5e94\u7528\u573a\u666f\u3001\u7f16\u7a0b\u8bed\u8a00\u3001\u5e93\u73af\u5883\u751a\u81f3\u4e00\u4e9b\u793e\u4ea4\u56e0\u7d20\uff08\uff1f\uff09\u3002\u6700\u597d\u7684\u56de\u7b54\u53ef\u80fd\u5c31\u662f\u201c\u627e\u5230\u80fd\u7b26\u5408\u4f60\u9700\u6c42\u7684\u6700\u7b80\u5355\u7684\u65b9\u6848\u201d\u3002\\n\\n\u6709\u4e00\u4e9b JSON \u7684\u6269\u5c55\u683c\u5f0f\u66f4\u9002\u5408\u4e8e\u4eba\u7c7b\u8fdb\u884c\u7f16\u8f91\uff0c\u4f8b\u5982 JSON5\u3001Hjson \u548c HOCON\u3002\u8fd9\u4e9b\u770b\u8d77\u6765\u90fd\u662f\u666e\u901aJSON\u7684\u5408\u7406\u5347\u7ea7\uff0c\u5c3d\u7ba1\u6211\u81ea\u5df1\u6ca1\u6709\u4f7f\u7528\u8fc7\u5b83\u4eec\u3002\u7279\u522b\u662f JSON5 \u4f3c\u4e4e\u662f\u4e00\u4e2a\u4e0d\u9519\u7684\u9009\u62e9\uff0c\u56e0\u4e3a\u5b83\u5bf9 JSON \u7684\u6539\u52a8\u6700\u5c11\u3002\u6211\u4e0d\u80fd\u7ed9\u51fa\u5173\u4e8e\u8fd9\u4e9b\u6269\u5c55\u683c\u5f0f\u7684\u5efa\u8bae\uff0c\u56e0\u4e3a\u6211\u6ca1\u6709\u6240\u6709\u7684\u683c\u5f0f\u8fdb\u884c\u6df1\u5165\u7684\u6bd4\u8f83\u3002\u53ea\u662f\u770b\u4e00\u773c\u683c\u5f0f\u89c4\u8303\u5e76\u4e0d\u80fd\u53d1\u73b0\u6f5c\u5728\u7684\u7f3a\u70b9\uff08YAML \u5c31\u662f\u4e00\u4e2a\u5f88\u597d\u7684\u4f8b\u5b50\uff09\u3002\u6211\u6ca1\u6709\u65f6\u95f4\u6216\u662f\u5174\u8da3\u5bf9\u6240\u6709\u66ff\u4ee3\u65b9\u6848\u8fdb\u884c\u5168\u9762\u6df1\u5165\u7684\u5ba1\u67e5\u3002\\n\\n## \u540e\u8bb0\\n\\n\u8fd9\u662f\u6211\u7b2c\u4e00\u6b21\u505a\u9700\u8981\u53d1\u5e03\u5230\u7f51\u4e0a\u7684\u6bd4\u8f83\u6b63\u5f0f\u7684\u7ffb\u8bd1\u5de5\u4f5c\u3002\u867d\u7136\u6700\u65e9\u81ea\u5df1\u5728\u8bfb paper \u7684\u65f6\u5019\u56e0\u4e3a\u82f1\u8bed\u751f\u758f\uff0c\u4e5f\u4f1a\u8fb9\u8bfb\u8fb9\u7ffb\u8bd1\u4e00\u4e9b\uff0c\u4f46\u662f\u6bd5\u7adf\u90a3\u662f\u7ffb\u8bd1\u7ed9\u81ea\u5df1\u770b\u7684\uff0c\u53ea\u8981\u81ea\u5df1\u80fd\u770b\u61c2\u5c31\u884c\u4e86\uff0c\u4e5f\u4e0d\u7528\u8ffd\u6c42\u4ec0\u4e48\u8bed\u53e5\u901a\u987a\u4e4b\u7c7b\u7684\u3002\u7136\u800c\u8981\u53d1\u5e03\u51fa\u6765\u7684\u6587\u7ae0\u4e0d\u4e00\u6837\uff0c\u81f3\u5c11\u8981\u4fdd\u8bc1\u5927\u591a\u6570\u8bfb\u8005\u80fd\u591f\u770b\u5f97\u61c2\u3002\\n\\n\u6574\u7bc7\u7ffb\u5b8c\u56de\u8fc7\u5934\u770b\u770b\uff0c\u8fd8\u662f\u6709\u5f88\u591a\u751f\u786c\u4f3c\u673a\u7ffb\u7684\u5730\u65b9\uff0c\u4e3b\u8981\u539f\u56e0\u53ef\u80fd\u8fd8\u662f\u81ea\u5df1\u7684\u8868\u8fbe\u80fd\u529b\u4e0d\u591f\u3002\u7ffb\u8bd1\u6280\u672f\u6587\u7ae0\u5728\u6211\u770b\u6765\u662f\u4e2a\u5403\u529b\u4e0d\u8ba8\u597d\u7684\u6d3b\uff0c\u7ffb\u7684\u518d\u597d\u4e5f\u4e0d\u5982\u76f4\u63a5\u8bfb\u539f\u6587\u6765\u7684\u6e05\u6670\u3002\u81f3\u4e8e\u4e3a\u4ec0\u4e48\u8981\u505a\u8fd9\u6837\u7684\u4e8b\u60c5\uff0c \u6211\u60f3\u6709\u65f6\u95f4\u5355\u72ec\u5199\u4e00\u7bc7\u8c08\u4e00\u8c08\u3002\u76ee\u524d\u6765\u770b\uff0c\u5c31\u6743\u5f53\u662f\u5bf9\u4e8e\u81ea\u5df1\u8868\u8fbe\u80fd\u529b\u7684\u953b\u70bc\u5427\u3002\\n\\n[1]: https://www.arp242.net/json-config.html \\"The downsides of JSON for config files\\"\\n[2]: https://vorba.ch/2013/json-comments.html \\"Why are comments not allowed in JSON?\\""},{"id":"static-service","metadata":{"permalink":"/static-service","editUrl":"https://github.com/edwardzjl/edwardzjl.github.io/blob/main/blog/2019-07-04-static-service/index.md","source":"@site/blog/2019-07-04-static-service/index.md","title":"\u7cfb\u7edf\u4e2d\u72b6\u6001\u4e3a static \u7684\u670d\u52a1","description":"\u6700\u8fd1\u5f00\u59cb\u63a5\u89e6 Linux \u8fd0\u7ef4\u7684\u5de5\u4f5c\uff0c\u7b2c\u4e00\u4ef6\u4e8b\u60c5\u5c31\u662f\u770b\u770b\u7cfb\u7edf\u4e2d\u8dd1\u4e86\u591a\u5c11\u670d\u52a1\u3002","date":"2019-07-04T00:00:00.000Z","tags":[{"inline":true,"label":"linux","permalink":"/tags/linux"}],"readingTime":0.64,"hasTruncateMarker":true,"authors":[{"name":"Junlin Zhou","title":"Fullstack Engineer @ ZJU ICI","url":"https://github.com/edwardzjl","imageURL":"https://github.com/edwardzjl.png","key":"jlzhou","page":null}],"frontMatter":{"slug":"static-service","authors":["jlzhou"],"tags":["linux"]},"unlisted":false,"prevItem":{"title":"[\u8bd1] JSON\u683c\u5f0f\u4f5c\u4e3a\u914d\u7f6e\u6587\u4ef6\u7684\u7f3a\u70b9","permalink":"/the-downsides-of-json-for-config-files"},"nextItem":{"title":"[\u8bd1] javax.persistence.Id \u548c org.springframework.data.annotation.Id \u7684\u533a\u522b","permalink":"/difference-between-javax.persistence.id-and-org.springframework.data.annotation.id"}},"content":"\u6700\u8fd1\u5f00\u59cb\u63a5\u89e6 Linux \u8fd0\u7ef4\u7684\u5de5\u4f5c\uff0c\u7b2c\u4e00\u4ef6\u4e8b\u60c5\u5c31\u662f\u770b\u770b\u7cfb\u7edf\u4e2d\u8dd1\u4e86\u591a\u5c11\u670d\u52a1\u3002\\n\\n\x3c!-- truncate --\x3e\\n\\n\u96c6\u7fa4\u7528\u7684\u662f CentOS 7\uff0c\u53ef\u4ee5\u901a\u8fc7 ```bash systemctl list-unit-files``` \u8fd9\u4e2a\u547d\u4ee4\u67e5\u770b\u6240\u6709\u670d\u52a1\uff0c\u6572\u4e0b\u56de\u8f66\u540e\u6253\u5370\u51fa\u6765\u8fd9\u4e48\u4e00\u5806\u73a9\u5e94\u513f\uff1a\\n\\n![services](./services.png \\"services\\")\\n\\nservice \u7684 `disabled` \u548c `enabled` \u72b6\u6001\u90fd\u597d\u7406\u89e3\uff0c`static` \u662f\u4e2a\u5565\uff1f\u5728[\u4e0d\u5b58\u5728\u7684\u7f51\u7ad9][1]\u4e0a\u4e00\u987f\u67e5\u627e\uff0c\u627e\u5230\u5982\u4e0b\u8fd9\u756a\u89e3\u91ca\uff1a\\n\\n> \\"static\\" means \\"enabled because something else wants it\\". Think by analogy to pacman\'s package install reasons:\\n>\\n> - enabled :: explicitly installed\\n> - static :: installed as dependency\\n> - disabled :: not installed\\n\\n\u610f\u601d\u662f\uff0c\u72b6\u6001\u4e3a `static` \u7684\u670d\u52a1\uff0c\u662f\u4f5c\u4e3a\u522b\u7684\u670d\u52a1\u7684\u4f9d\u8d56\u800c\u5b58\u5728\u3002\\n\\n[1]: https://bbs.archlinux.org/viewtopic.php?id=147964 \\"systemd \'static\' unit file state\\""},{"id":"difference-between-javax.persistence.id-and-org.springframework.data.annotation.id","metadata":{"permalink":"/difference-between-javax.persistence.id-and-org.springframework.data.annotation.id","editUrl":"https://github.com/edwardzjl/edwardzjl.github.io/blob/main/blog/2019-06-27-difference-between-javax.persistence.id-and-org.springframework.data.annotation.id/index.md","source":"@site/blog/2019-06-27-difference-between-javax.persistence.id-and-org.springframework.data.annotation.id/index.md","title":"[\u8bd1] javax.persistence.Id \u548c org.springframework.data.annotation.Id \u7684\u533a\u522b","description":"org.springframework.data.annotation.Id","date":"2019-06-27T00:00:00.000Z","tags":[{"inline":true,"label":"spring","permalink":"/tags/spring"},{"inline":true,"label":"java","permalink":"/tags/java"}],"readingTime":0.45,"hasTruncateMarker":true,"authors":[{"name":"Junlin Zhou","title":"Fullstack Engineer @ ZJU ICI","url":"https://github.com/edwardzjl","imageURL":"https://github.com/edwardzjl.png","key":"jlzhou","page":null}],"frontMatter":{"slug":"difference-between-javax.persistence.id-and-org.springframework.data.annotation.id","authors":["jlzhou"],"tags":["spring","java"]},"unlisted":false,"prevItem":{"title":"\u7cfb\u7edf\u4e2d\u72b6\u6001\u4e3a static \u7684\u670d\u52a1","permalink":"/static-service"},"nextItem":{"title":"Install postgres on OSX","permalink":"/install-postgres-on-osx"}},"content":"## org.springframework.data.annotation.Id\\n\\n`org.springframework.data.annotation.Id` \u662f Spring \u5b9a\u4e49\u7684 annotation\uff0c\u7528\u6765\u652f\u6301 \\"\u6ca1\u6709\u50cf JPA \u90a3\u6837\u7684\u6301\u4e45\u5316 API\\" \u7684\u975e\u5173\u7cfb\u578b\u6570\u636e\u5e93\u6216\u662f\u6846\u67b6\u7684\u6301\u4e45\u5316\uff0c\u56e0\u6b64\u5b83\u5e38\u88ab\u7528\u4e8e\u5176\u5b83 spring-data \u9879\u76ee\uff0c\u4f8b\u5982 spring-data-mongodb \u548c spring-data-solr \u7b49\u3002\\n\\n## javax.persistence.Id\\n\\n`javax.persistence.Id` \u662f\u7531 JPA \u5b9a\u4e49\u7684 annotation\uff0cJPA \u4ec5\u9002\u7528\u4e8e\u5173\u7cfb\u6570\u636e\u7684\u7ba1\u7406\u3002\\n\\n\x3c!-- truncate --\x3e\\n\\n## Ref\\n\\n- [whats-the-difference-between-javax-persistence-id-and-org-springframework-data](https://stackoverflow.com/questions/39643960/whats-the-difference-between-javax-persistence-id-and-org-springframework-data)"},{"id":"install-postgres-on-osx","metadata":{"permalink":"/install-postgres-on-osx","editUrl":"https://github.com/edwardzjl/edwardzjl.github.io/blob/main/blog/2019-04-13-install-postgres-on-osx/index.md","source":"@site/blog/2019-04-13-install-postgres-on-osx/index.md","title":"Install postgres on OSX","description":"If you installed Postgres from homebrew, the default user postgres isn\'t automatically created, you need to run following command in your terminal:","date":"2019-04-13T00:00:00.000Z","tags":[{"inline":true,"label":"postgres","permalink":"/tags/postgres"},{"inline":true,"label":"osx","permalink":"/tags/osx"}],"readingTime":0.17,"hasTruncateMarker":true,"authors":[{"name":"Junlin Zhou","title":"Fullstack Engineer @ ZJU ICI","url":"https://github.com/edwardzjl","imageURL":"https://github.com/edwardzjl.png","key":"jlzhou","page":null}],"frontMatter":{"slug":"install-postgres-on-osx","authors":["jlzhou"],"tags":["postgres","osx"]},"unlisted":false,"prevItem":{"title":"[\u8bd1] javax.persistence.Id \u548c org.springframework.data.annotation.Id \u7684\u533a\u522b","permalink":"/difference-between-javax.persistence.id-and-org.springframework.data.annotation.id"},"nextItem":{"title":"Ubuntu / CentOS \u7f16\u8bd1\u5b89\u88c5\u5e26\u526a\u8d34\u677f\u652f\u6301\u7684 Vim","permalink":"/config-vim-8-clipboard"}},"content":"If you installed Postgres from homebrew, the default user `postgres` isn\'t automatically created, you need to run following command in your terminal:\\n\\n\x3c!-- truncate --\x3e\\n\\n```sh\\n/Applications/Postgres.app/Contents/Versions/9.*/bin/createuser -s postgres\\n```"},{"id":"config-vim-8-clipboard","metadata":{"permalink":"/config-vim-8-clipboard","editUrl":"https://github.com/edwardzjl/edwardzjl.github.io/blob/main/blog/2019-03-14-config-vim-8-clipboard/index.md","source":"@site/blog/2019-03-14-config-vim-8-clipboard/index.md","title":"Ubuntu / CentOS \u7f16\u8bd1\u5b89\u88c5\u5e26\u526a\u8d34\u677f\u652f\u6301\u7684 Vim","description":"\u901a\u8fc7 Ubuntu \u6216 CentOS \u7cfb\u7edf\u81ea\u5e26\u7684\u8f6f\u4ef6\u6e90\u5b89\u88c5 Vim\uff0c\u5f80\u5f80\u53ea\u80fd\u5f97\u5230\u8f83\u65e7\u7684\u7248\u672c\uff08\u901a\u5e38\u662f 7.4.x\uff09\u3002\u800c\u4ece Vim 8.0 \u5f00\u59cb\uff0c\u5b98\u7f51\u63a8\u8350\u7684\u5b89\u88c5\u65b9\u5f0f\u662f\u901a\u8fc7 Git \u514b\u9686\u6e90\u7801\u81ea\u884c\u7f16\u8bd1\u3002","date":"2019-03-14T00:00:00.000Z","tags":[{"inline":true,"label":"vim","permalink":"/tags/vim"}],"readingTime":1.09,"hasTruncateMarker":true,"authors":[{"name":"Junlin Zhou","title":"Fullstack Engineer @ ZJU ICI","url":"https://github.com/edwardzjl","imageURL":"https://github.com/edwardzjl.png","key":"jlzhou","page":null}],"frontMatter":{"slug":"config-vim-8-clipboard","authors":["jlzhou"],"tags":["vim"]},"unlisted":false,"prevItem":{"title":"Install postgres on OSX","permalink":"/install-postgres-on-osx"}},"content":"\u901a\u8fc7 Ubuntu \u6216 CentOS \u7cfb\u7edf\u81ea\u5e26\u7684\u8f6f\u4ef6\u6e90\u5b89\u88c5 Vim\uff0c\u5f80\u5f80\u53ea\u80fd\u5f97\u5230\u8f83\u65e7\u7684\u7248\u672c\uff08\u901a\u5e38\u662f 7.4.x\uff09\u3002\u800c\u4ece Vim 8.0 \u5f00\u59cb\uff0c\u5b98\u7f51\u63a8\u8350\u7684\u5b89\u88c5\u65b9\u5f0f\u662f\u901a\u8fc7 Git \u514b\u9686\u6e90\u7801\u81ea\u884c\u7f16\u8bd1\u3002\\n\\n\u4e0d\u8fc7\u9700\u8981\u6ce8\u610f\uff0c**\u9ed8\u8ba4\u7f16\u8bd1\u51fa\u6765\u7684 Vim \u5e76\u4e0d\u5305\u542b\u526a\u8d34\u677f\u652f\u6301\uff08clipboard support\uff09**\uff0c\u56e0\u6b64\u65e0\u6cd5\u4e0e\u7cfb\u7edf\u526a\u8d34\u677f\u4ea4\u4e92\uff08\u4f8b\u5982\u590d\u5236\u7c98\u8d34\u5230\u5176\u4ed6\u7a0b\u5e8f\uff09\u3002\\n\\n\x3c!-- truncate --\x3e\\n\\n\u8981\u5728\u7f16\u8bd1\u65f6\u542f\u7528\u526a\u8d34\u677f\u652f\u6301\uff0c\u81f3\u5c11\u9700\u8981\u4e24\u4e2a\u4f9d\u8d56\u5305\uff1a\\n\\n* `libx11-dev`\uff1a\u63d0\u4f9b Xorg \u7684\u5934\u6587\u4ef6\uff08xorg header files\uff09\\n* `dbus-x11`\uff1a\u63d0\u4f9b X11 \u7684 D-Bus \u652f\u6301\\n\\n\u4f60\u53ef\u4ee5\u5728 [https://packages.ubuntu.com](https://packages.ubuntu.com) \u641c\u7d22\u5177\u4f53\u7684\u4f9d\u8d56\u9879\u4f4d\u7f6e\uff0c\u6700\u7ec8\u786e\u8ba4\u8fd9\u4e24\u4e2a\u5305\u5c31\u662f\u6211\u4eec\u6240\u9700\u7684\u3002\\n\\n\u5b89\u88c5\u4f9d\u8d56\u5e76\u7f16\u8bd1 Vim \u7684\u5b8c\u6574\u6d41\u7a0b\u5982\u4e0b\uff1a\\n\\n```sh\\nsudo apt-get install libx11-dev dbus-x11\\ngit clone https://github.com/vim/vim.git\\ncd vim\\n./configure --with-features=huge --enable-gui=auto --enable-cscope --prefix=/usr/local\\nmake\\nsudo make install\\n```\\n\\n> \u5176\u4e2d `--with-features=huge` \u542f\u7528\u51e0\u4e4e\u6240\u6709\u529f\u80fd\uff0c`--enable-gui=auto` \u53ef\u9009\u542f\u7528 GUI \u6a21\u5f0f\uff08\u5982 gvim\uff09\uff0c`--enable-cscope` \u5219\u7528\u4e8e\u589e\u5f3a\u4ee3\u7801\u5bfc\u822a\u529f\u80fd\u3002\\n\\n\u5b89\u88c5\u5b8c\u6210\u540e\uff0c\u4f60\u53ef\u4ee5\u4f7f\u7528 `vim --version` \u68c0\u67e5\u662f\u5426\u542f\u7528\u4e86 `+clipboard`\uff0c\u786e\u8ba4\u526a\u8d34\u677f\u652f\u6301\u662f\u5426\u751f\u6548\u3002"}]}}')}}]);