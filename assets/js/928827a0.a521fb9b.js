"use strict";(self.webpackChunkedwardzjl_github_io=self.webpackChunkedwardzjl_github_io||[]).push([[244],{6323:t=>{t.exports=JSON.parse('{"permalink":"/distributed-sft-part-2","editUrl":"https://github.com/edwardzjl/edwardzjl.github.io/blob/main/blog/2025-02-07-distributed-sft-part-2/index.md","source":"@site/blog/2025-02-07-distributed-sft-part-2/index.md","title":"Distributed SFT Part 2: Scaling Locally","description":"Introduction","date":"2025-02-07T00:00:00.000Z","tags":[{"inline":true,"label":"LLM","permalink":"/tags/llm"},{"inline":true,"label":"distributed-training","permalink":"/tags/distributed-training"}],"readingTime":12.72,"hasTruncateMarker":true,"authors":[{"name":"Junlin Zhou","title":"Fullstack Engineer @ ZJU ICI","url":"https://github.com/edwardzjl","imageURL":"https://github.com/edwardzjl.png","key":"jlzhou","page":null}],"frontMatter":{"slug":"distributed-sft-part-2","authors":["jlzhou"],"tags":["LLM","distributed-training"]},"unlisted":false,"nextItem":{"title":"Distributed SFT Part 1: Starting Locally","permalink":"/distributed-sft-part-1"}}')},6874:(t,e,n)=>{n.r(e),n.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>c,frontMatter:()=>a,metadata:()=>i,toc:()=>d});var i=n(6323),r=n(4848),s=n(8453);const a={slug:"distributed-sft-part-2",authors:["jlzhou"],tags:["LLM","distributed-training"]},o="Distributed SFT Part 2: Scaling Locally",l={authorsImageUrls:[void 0]},d=[{value:"Introduction",id:"introduction",level:2}];function u(t){const e={a:"a",code:"code",h2:"h2",p:"p",...(0,s.R)(),...t.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.a,{href:"https://edwardzjl.github.io/distributed-sft-part-1",children:"In the first part of this series"}),", we covered the basics of setting up a local SFT experiment using ",(0,r.jsx)(e.code,{children:"trl"}),". We learned how to format datasets for ",(0,r.jsx)(e.code,{children:"trl"}),"'s ",(0,r.jsx)(e.code,{children:"SFTTrainer"})," and preprocess them to fit the required structure."]}),"\n",(0,r.jsx)(e.p,{children:"Now, it's time to take the next step. In this post, we'll focus on scaling the SFT setup to handle larger tasks. Specifically, we'll explore how to fine-tune an LLM in a single-node, multi-GPU environment. Along the way, we'll discuss optimization techniques to reduce memory usage, speed up training, and enable fine-tuning of even larger models. Let's get started!"})]})}function c(t={}){const{wrapper:e}={...(0,s.R)(),...t.components};return e?(0,r.jsx)(e,{...t,children:(0,r.jsx)(u,{...t})}):u(t)}},8453:(t,e,n)=>{n.d(e,{R:()=>a,x:()=>o});var i=n(6540);const r={},s=i.createContext(r);function a(t){const e=i.useContext(s);return i.useMemo(function(){return"function"==typeof t?t(e):{...e,...t}},[e,t])}function o(t){let e;return e=t.disableParentContext?"function"==typeof t.components?t.components(r):t.components||r:a(t.components),i.createElement(s.Provider,{value:e},t.children)}}}]);