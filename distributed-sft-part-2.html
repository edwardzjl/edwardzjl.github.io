<!doctype html>
<html lang="zh-Hans" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.8.1">
<title data-rh="true">Distributed SFT Part 2: Scaling Locally | Edwardzjl</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://edwardzjl.github.io/distributed-sft-part-2"><meta data-rh="true" property="og:locale" content="zh_Hans"><meta data-rh="true" property="og:locale:alternate" content="en"><meta data-rh="true" name="docusaurus_locale" content="zh-Hans"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="zh-Hans"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="Distributed SFT Part 2: Scaling Locally | Edwardzjl"><meta data-rh="true" name="description" content="Introduction"><meta data-rh="true" property="og:description" content="Introduction"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2025-02-07T00:00:00.000Z"><meta data-rh="true" property="article:author" content="https://github.com/edwardzjl"><meta data-rh="true" property="article:tag" content="LLM,distributed-training"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://edwardzjl.github.io/distributed-sft-part-2"><link data-rh="true" rel="alternate" href="https://edwardzjl.github.io/en/distributed-sft-part-2" hreflang="en"><link data-rh="true" rel="alternate" href="https://edwardzjl.github.io/distributed-sft-part-2" hreflang="zh-Hans"><link data-rh="true" rel="alternate" href="https://edwardzjl.github.io/distributed-sft-part-2" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","@id":"https://edwardzjl.github.io/distributed-sft-part-2","mainEntityOfPage":"https://edwardzjl.github.io/distributed-sft-part-2","url":"https://edwardzjl.github.io/distributed-sft-part-2","headline":"Distributed SFT Part 2: Scaling Locally","name":"Distributed SFT Part 2: Scaling Locally","description":"Introduction","datePublished":"2025-02-07T00:00:00.000Z","author":{"@type":"Person","name":"Junlin Zhou","description":"Fullstack Engineer @ ZJU ICI","url":"https://github.com/edwardzjl","image":"https://github.com/edwardzjl.png"},"keywords":[],"isPartOf":{"@type":"Blog","@id":"https://edwardzjl.github.io/","name":"Blog"}}</script><link rel="alternate" type="application/rss+xml" href="/rss.xml" title="Edwardzjl RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/atom.xml" title="Edwardzjl Atom Feed"><link rel="stylesheet" href="/assets/css/styles.66e0c820.css">
<script src="/assets/js/runtime~main.e1b5de30.js" defer="defer"></script>
<script src="/assets/js/main.e665ba3d.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t="light";var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",e||t),document.documentElement.setAttribute("data-theme-choice",e||t)}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/img/logo.svg"><link rel="preload" as="image" href="https://github.com/edwardzjl.png"><div role="region" aria-label="Ë∑≥Âà∞‰∏ªË¶ÅÂÜÖÂÆπ"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Ë∑≥Âà∞‰∏ªË¶ÅÂÜÖÂÆπ</a></div><nav aria-label="‰∏ªÂØºËà™" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="ÂàáÊç¢ÂØºËà™Ê†è" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">My Site</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/">Home</a><a class="navbar__item navbar__link" href="/tags">Tags</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/edwardzjl/edwardzjl.github.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Feed</a><ul class="dropdown__menu"><li><a href="https://edwardzjl.github.io/rss.xml" target="_blank" rel="noopener noreferrer" class="dropdown__link">RSS</a></li><li><a href="https://edwardzjl.github.io/atom.xml" target="_blank" rel="noopener noreferrer" class="dropdown__link">Atom</a></li></ul></div><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="ÂàáÊç¢ÊµÖËâ≤/ÊöóÈªëÊ®°ÂºèÔºàÂΩìÂâç‰∏∫system modeÔºâ"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="ÊúÄËøëÂçöÊñáÂØºËà™"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><div role="group"><h3 class="yearGroupHeading_rMGB">2025</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/distributed-sft-part-2">Distributed SFT Part 2: Scaling Locally</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/distributed-sft-part-1">Distributed SFT Part 1: Starting Locally</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2019</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/the-downsides-of-json-for-config-files">[ËØë] JSONÊ†ºÂºè‰Ωú‰∏∫ÈÖçÁΩÆÊñá‰ª∂ÁöÑÁº∫ÁÇπ</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/static-service">Á≥ªÁªü‰∏≠Áä∂ÊÄÅ‰∏∫ static ÁöÑÊúçÂä°</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/difference-between-javax.persistence.id-and-org.springframework.data.annotation.id">[ËØë] javax.persistence.Id Âíå org.springframework.data.annotation.Id ÁöÑÂå∫Âà´</a></li></ul></div></nav></aside><main class="col col--7"><article class=""><header><h1 class="title_f1Hy">Distributed SFT Part 2: Scaling Locally</h1><div class="container_mt6G margin-vert--md"><time datetime="2025-02-07T00:00:00.000Z">2025Âπ¥2Êúà7Êó•</time> ¬∑ <!-- -->ÈòÖËØªÈúÄ 13 ÂàÜÈíü</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/edwardzjl" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://github.com/edwardzjl.png" alt="Junlin Zhou"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://github.com/edwardzjl" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Junlin Zhou</span></a></div><small class="authorTitle_nd0D" title="Fullstack Engineer @ ZJU ICI">Fullstack Engineer @ ZJU ICI</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div id="__blog-post-container" class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="introduction">Introduction<a href="#introduction" class="hash-link" aria-label="IntroductionÁöÑÁõ¥Êé•ÈìæÊé•" title="IntroductionÁöÑÁõ¥Êé•ÈìæÊé•">‚Äã</a></h2>
<p><a href="https://edwardzjl.github.io/distributed-sft-part-1" target="_blank" rel="noopener noreferrer">In the first part of this series</a>, we covered the basics of setting up a local SFT experiment using <code>trl</code>. We learned how to format datasets for <code>trl</code>&#x27;s <code>SFTTrainer</code> and preprocess them to fit the required structure.</p>
<p>Now, it&#x27;s time to take the next step. In this post, we&#x27;ll focus on scaling the SFT setup to handle larger tasks. Specifically, we&#x27;ll explore how to fine-tune an LLM in a single-node, multi-GPU environment. Along the way, we&#x27;ll discuss optimization techniques to reduce memory usage, speed up training, and enable fine-tuning of even larger models. Let&#x27;s get started!</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="prerequisites">Prerequisites<a href="#prerequisites" class="hash-link" aria-label="PrerequisitesÁöÑÁõ¥Êé•ÈìæÊé•" title="PrerequisitesÁöÑÁõ¥Êé•ÈìæÊé•">‚Äã</a></h2>
<p>To follow along with this tutorial, you&#x27;ll need a machine equipped with multiple NVIDIA GPUs. Ensure that the GPUs are connected via high-speed interconnects to minimize communication overhead. For reference, I ran this experiment using 8 NVIDIA V100 SXM2 GPUs.</p>
<p><strong>Important Considerations:</strong></p>
<ol>
<li>
<p>GPU Architecture: While I ran this experiment with V100 GPUs, newer architectures like Ampere or Hopper are strongly recommended. These GPUs offers advanced features, such as support for more efficient precision types and improved communication speeds. Additionally, techniques like <a href="https://github.com/Dao-AILab/flash-attention" target="_blank" rel="noopener noreferrer">flash-attention</a> are <a href="https://github.com/Dao-AILab/flash-attention/issues/524" target="_blank" rel="noopener noreferrer">only compatible with Ampere or newer GPUs</a>.</p>
</li>
<li>
<p>Interconnect Quality: Verify GPU communication bandwidth using <code>nvidia-smi topo -m</code>. Poor interconnects can become a bottleneck during training.</p>
</li>
</ol>
<p>Additionally, you&#x27;ll need to install the following dependencies:</p>
<div class="language-txt codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-txt codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">datasets</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">torch</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">transformers</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">trl</span><br></span></code></pre></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="tuning-hyperparameters">Tuning Hyperparameters<a href="#tuning-hyperparameters" class="hash-link" aria-label="Tuning HyperparametersÁöÑÁõ¥Êé•ÈìæÊé•" title="Tuning HyperparametersÁöÑÁõ¥Êé•ÈìæÊé•">‚Äã</a></h2>
<p><a href="https://huggingface.co/datasets/BAAI/Infinity-Instruct" target="_blank" rel="noopener noreferrer">BAAI/Infinity-Instruct</a> provides several officially fine-tuned models, including <a href="https://huggingface.co/BAAI/Infinity-Instruct-7M-Gen-Llama3_1-70B" target="_blank" rel="noopener noreferrer">Llama3.1-70B</a>, <a href="https://huggingface.co/BAAI/Infinity-Instruct-7M-Gen-mistral-7B" target="_blank" rel="noopener noreferrer">mistral-7B</a>, <a href="https://huggingface.co/BAAI/Infinity-Instruct-3M-0625-Qwen2-7B" target="_blank" rel="noopener noreferrer">Qwen2-7B</a> and <a href="https://huggingface.co/BAAI/Infinity-Instruct-3M-0625-Yi-1.5-9B" target="_blank" rel="noopener noreferrer">Yi-1.5-9B</a>. They also generously share the training details for these models.</p>
<p>For this tutorial, we&#x27;ll use the <a href="https://huggingface.co/BAAI/Infinity-Instruct-3M-0625-Qwen2-7B#training-details" target="_blank" rel="noopener noreferrer">hyperparameters for Qwen2-7B</a> as a reference. Here&#x27;s how these hyperparameters map to training arguments in <code>trl</code>:</p>
<ul>
<li>epoch: <code>--num_train_epochs</code></li>
<li>lr: <code>--learning_rate</code></li>
<li>lr_warmup_steps: <code>--warmup_steps</code></li>
<li>lr_decay_style: <code>--lr_scheduler_type</code> (Set to <code>cosine_with_min_lr</code> along with <code>min_lr</code>. Available scheduler options can be found <a href="https://huggingface.co/docs/transformers/main_classes/optimizer_schedules#transformers.SchedulerType" target="_blank" rel="noopener noreferrer">here</a>.)</li>
<li>min_lr: <code>--lr_scheduler_kwargs</code> (Set to <code>&quot;{\&quot;min_lr\&quot;: 0}&quot;</code>. This argument isn&#x27;t clearly documented; I discovered it through <a href="https://github.com/huggingface/transformers/pull/29341" target="_blank" rel="noopener noreferrer">this PR</a> and <a href="https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/tests/trainer/test_trainer.py#L1065" target="_blank" rel="noopener noreferrer">this test case</a>.)</li>
<li>weight_decay: <code>--weight_decay</code></li>
<li>adam_beta1: <code>--adam_beta1</code></li>
<li>adam_beta2: <code>--adam_beta2</code></li>
<li>clip_grad: <code>--max_grad_norm</code></li>
</ul>
<p>One additional parameter worth mentioning is the <code>global_batch_size</code>, which isn&#x27;t directly set in the training script. The global batch size is determined by the equation:</p>
<p><code>global_batch_size = per_device_train_batch_size * gradient_accumulation_steps * numGPUs</code>.</p>
<p>For example, if our target global batch size is 528 and we&#x27;re using 8 GPUs, the local batch size (per GPU) would be:</p>
<p><code>528 / 8 = 66</code>.</p>
<p>If we can fit 2 samples per batch in each GPU, we can then set <code>per_device_train_batch_size</code> to 2 and <code>gradient_accumulation_steps</code> to 33.</p>
<p>Another important consideration is training precision. Modern GPUs (Ampere series or newer) supports <code>bf16</code> and <code>tf32</code>, while older GPUs only support <code>fp16</code> and <code>fp32</code>. When fine-tuning, make sure the precision matches that of the base model. Specifically, avoid using <code>fp16</code> if the base model was trained with <code>bf16</code>. For more details, refer to <a href="https://github.com/huggingface/transformers/pull/10956" target="_blank" rel="noopener noreferrer">this PR</a>.</p>
<p>You can find the data type of your base model by the <code>torch_dtype</code> field in the <code>config.json</code> file. So if you&#x27;re fine-tuning a <code>bf16</code> model but don&#x27;t have access to Ampere or newer GPUs (like me), it&#x27;s best to stick with <code>fp32</code> for now.</p>
<p>Now that we&#x27;ve covered the essential hyperparameters and considerations, let&#x27;s move on to some optimization techniques that will help improve training efficiency and resource usage.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="gradient-accumulation">Gradient Accumulation<a href="#gradient-accumulation" class="hash-link" aria-label="Gradient AccumulationÁöÑÁõ¥Êé•ÈìæÊé•" title="Gradient AccumulationÁöÑÁõ¥Êé•ÈìæÊé•">‚Äã</a></h3>
<p>You may have noticed that I used <code>per_device_train_batch_size</code> and <code>gradient_accumulation_steps</code> to calculate the local batch size. Gradient accumulation allows you to accumulate gradients over multiple mini-batches before updating the model. This technique is particularly useful when the desired batch size exceeds your hardware&#x27;s memory capacity.</p>
<p>As a general guideline:</p>
<ul>
<li>Use the largest <code>per_device_train_batch_size</code> that fits within your VRAM</li>
<li>Adjust <code>gradient_accumulation_steps</code> to achieve your target batch size if necessary.</li>
</ul>
<p>This way, you can effectively simulate a larger batch size without running into memory limitations</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="gradient-checkpointing">Gradient Checkpointing<a href="#gradient-checkpointing" class="hash-link" aria-label="Gradient CheckpointingÁöÑÁõ¥Êé•ÈìæÊé•" title="Gradient CheckpointingÁöÑÁõ¥Êé•ÈìæÊé•">‚Äã</a></h3>
<p>Gradient checkpointing is a memory optimization technique that reduces memory usage by trading off computation. During training, a large portion of memory is used to store intermediate activations for backpropagation. Gradient checkpointing reduces this memory usage by selectively saving a subset of activations and recomputing the rest during the backward pass.</p>
<p>Note: According to <a href="https://pytorch.org/docs/stable/checkpoint.html" target="_blank" rel="noopener noreferrer">https://pytorch.org/docs/stable/checkpoint.html</a>:</p>
<blockquote>
<p>There are currently two checkpointing implementations available, determined by the <code>use_reentrant</code> parameter. It is recommended that you use <code>use_reentrant=False</code>.</p>
</blockquote>
<p>You can read that section for a deeper understanding of  the differences between the two implementations.</p>
<p>At the time of writing, the <code>transformers</code> library (v4.48.1) <a href="https://github.com/huggingface/transformers/blob/2e752ead46a8845e8a160d2043c1336447895690/src/transformers/modeling_utils.py#L2538" target="_blank" rel="noopener noreferrer">uses the reentrant implementation by default</a>. To use the non-reentrant version, you must explicitly pass the following argument:</p>
<div class="language-sh codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-sh codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">--gradient_checkpointing_kwargs &quot;{\&quot;use_reentrant\&quot;: false}&quot;</span><br></span></code></pre></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="zero">ZeRO<a href="#zero" class="hash-link" aria-label="ZeROÁöÑÁõ¥Êé•ÈìæÊé•" title="ZeROÁöÑÁõ¥Êé•ÈìæÊé•">‚Äã</a></h3>
<p>There are several approaches to parallelizing training tasks, including <strong>Data Parallelism (DP)</strong>, <strong>Tensor Parallelism (TP)</strong>, <strong>Pipeline Parallelism (PP)</strong>, <strong>Zero Redundancy Optimizer (ZeRO)</strong>, <strong>Sequence Parallelism</strong> and <strong>Expert Parallelism</strong>. For a detailed overview of these methods, I recommend checking out <a href="https://github.com/stas00/ml-engineering/tree/master/training/model-parallelism#scalability-concepts" target="_blank" rel="noopener noreferrer">this excellent resource</a>.</p>
<p>In this tutorial, we&#x27;ll focus on <strong>ZeRO</strong>, which provides greater efficiency than traditional DP without requiring modifications to the training code.</p>
<p>ZeRO (Zero Redundancy Optimizer) is a powerful technique for scaling training by reducing memory usage. If you&#x27;re new to ZeRO, check out <a href="https://arxiv.org/abs/1910.02054" target="_blank" rel="noopener noreferrer">the original paper</a> or <a href="https://github.com/stas00/ml-engineering/tree/master/training/model-parallelism#zero-data-parallelism" target="_blank" rel="noopener noreferrer">this detailed article</a>.</p>
<p>ZeRO has three stages, each targeting different aspects  of memory savings:</p>
<ul>
<li>Stage 1: Reduces optimizer state memory.</li>
<li>Stage 2: Further reduces gradient memory.</li>
<li>Stage 3: Fully partitions model states, achieving the highest memory savings at the cost of increased communication overhead.</li>
</ul>
<p>Stage 3 provides the greatest memory efficiency but can significantly slow down training if inter-GPU communication is not fast enough. As a general guideline:</p>
<ul>
<li>Start with Stage 2.</li>
<li>Try Stage 3 only if Stage 2 still leads to CUDA OOM.</li>
</ul>
<p>That being said, it‚Äôs always worth testing both Stage 2 and Stage 3 on your setup to determine which one performs better on your hardware.</p>
<p>For this tutorial, we will use the official implementation of ZeRO -- <a href="https://www.deepspeed.ai/" target="_blank" rel="noopener noreferrer">DeepSpeed</a>. To use DeepSpeed, you&#x27;ll need to install it first. DeepSpeed provides C++/CUDA ops that can be pre-installed or JIT-compiled. If you choose the pre-installation option, refer to <a href="https://www.deepspeed.ai/tutorials/advanced-install/#pre-install-deepspeed-ops" target="_blank" rel="noopener noreferrer">this documentation</a>. we‚Äôll install DeepSpeed using the JIT method by running:</p>
<div class="language-sh codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-sh codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip install deepspeed</span><br></span></code></pre></div></div>
<p>The Hugging Face <code>transformers</code> library provides built-in support for DeepSpeed. You can enable it by specifying a DeepSpeed config file using the <code>--deepspeed</code> flag in your training script. For more information, refer to the <a href="https://huggingface.co/docs/transformers/deepspeed" target="_blank" rel="noopener noreferrer">DeepSpeed documentation in transformers</a>.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="liger-kernel">Liger Kernel<a href="#liger-kernel" class="hash-link" aria-label="Liger KernelÁöÑÁõ¥Êé•ÈìæÊé•" title="Liger KernelÁöÑÁõ¥Êé•ÈìæÊé•">‚Äã</a></h3>
<p><a href="https://github.com/linkedin/Liger-Kernel" target="_blank" rel="noopener noreferrer">Liger Kernel</a> is a collection of Triton kernels developed by LinkedIn to reduce memory usage and increase training throughput. The best part is that it requires no complex configuration, making it an easy addition to your setup. To install it, run:</p>
<div class="language-sh codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-sh codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip install liger-kernel</span><br></span></code></pre></div></div>
<p>Once installed, add the <code>--use_liger</code> flag to your training script, and you&#x27;ll automatically save VRAM without any extra setup or hassle. It&#x27;s a straightforward way to optimize your training without sacrificing performance.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="sample-packing">Sample Packing<a href="#sample-packing" class="hash-link" aria-label="Sample PackingÁöÑÁõ¥Êé•ÈìæÊé•" title="Sample PackingÁöÑÁõ¥Êé•ÈìæÊé•">‚Äã</a></h3>
<p>Large models are trained on GPUs to leverage their parallelism. However, in the context of language models, where we train on text sequences, the length of each sample varies.</p>
<p>The traditional approach to handle variable-length sequences is to pad each sample to match the longest one in a batch. While this ensures uniform input dimensions, it also leads to considerable memory waste due to the padding.</p>
<p><a href="https://arxiv.org/abs/2407.09105" target="_blank" rel="noopener noreferrer">Sample packing</a> addresses this issue by combining shorter samples into a single sequence. This technique allows for more efficient GPU memory usage, reducing waste and potentially speeding up training.</p>
<p>While the concept is straightforward, Implementing it correctly can be one of the most challenging tasks of this experiment for me.</p>
<p>At the first glance, <a href="https://huggingface.co/docs/trl/sft_trainer#packing-dataset--constantlengthdataset-" target="_blank" rel="noopener noreferrer">trl supports packing dataset by simply passing an argument</a>. However, uppon <a href="https://github.com/huggingface/trl/blob/f34b70a32ef2820d3fd5c5b1ff6d1fd1e7799f04/trl/trainer/sft_trainer.py#L459" target="_blank" rel="noopener noreferrer">further investigation</a>, I found that the implementation might not suit my needs. As pointed out in <a href="https://github.com/huggingface/trl/issues/805" target="_blank" rel="noopener noreferrer">this issue</a>, the attention mask is not handled properly, which can lead to potential cross contamination in attention between sequences. The following image illustrates this issue clearly. On the left is the result of using <code>--packing</code>, and on the right is the correct way to pack samples:</p>
<p><img decoding="async" loading="lazy" src="https://cdn-uploads.huggingface.co/production/uploads/63eb008e5c837d9968f1eb71/lzpKqOADV5mdOdclPbQ9C.png" alt="image/png" class="img_ev3q"></p>
<p>After further digging, I found that, at least for now, <a href="https://github.com/huggingface/transformers/issues/27640#issuecomment-2619471784" target="_blank" rel="noopener noreferrer">&#x27;the correct way of packing&#x27; is supported only with Flash Attention</a>. If you don&#x27;t have access to Ampere or newer GPUs, you may need to stick with the traditional padding approach.</p>
<p>However, if you&#x27;re lucky enough to have those GPUs, you can follow <a href="https://huggingface.co/blog/packing-with-FA2" target="_blank" rel="noopener noreferrer">this blog post</a> to enable sample packing during training. Note that I haven&#x27;t personally validated this approach. Also, as of writing, there are some PRs related to this feature that aren&#x27;t released yet (for example <a href="https://github.com/huggingface/trl/pull/2158" target="_blank" rel="noopener noreferrer">this one</a>). To access this functionality, you may need to install <code>trl</code> and <code>transformers</code> from source:</p>
<div class="language-sh codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-sh codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip install git+https://github.com/huggingface/trl</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">pip install git+https://github.com/huggingface/tranformers</span><br></span></code></pre></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="distributed-training">Distributed Training<a href="#distributed-training" class="hash-link" aria-label="Distributed TrainingÁöÑÁõ¥Êé•ÈìæÊé•" title="Distributed TrainingÁöÑÁõ¥Êé•ÈìæÊé•">‚Äã</a></h2>
<p>With all the optimizations in place, we&#x27;re now ready to scale our SFT experiment across multiple GPUs. To do so, we can use tools like <a href="https://pytorch.org/docs/stable/elastic/run.html" target="_blank" rel="noopener noreferrer">torchrun</a>, <a href="https://www.deepspeed.ai/getting-started/" target="_blank" rel="noopener noreferrer">deepspeed</a> or <a href="https://huggingface.co/docs/accelerate/index" target="_blank" rel="noopener noreferrer">accelerate</a>. Personally I prefer <code>torchrun</code> for its simplicity and ease of use.</p>
<p>By running the following command, we can distribute the training job across multiple GPUs:</p>
<p>Oh, and don&#x27;t forget to set up <code>wandb</code> for logging ‚Äî we&#x27;re doing proper fine-tuning now! üòâ</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>sft2.sh</summary><div><div class="collapsibleContent_i85q"><div class="language-sh codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-sh codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">torchrun \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--nproc_per_node 8 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">sft.py \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--model_name_or_path Qwen/Qwen2.5-3B \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--dataset_name BAAI/Infinity-Instruct \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--dataset_config 0625 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--do_train \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--learning_rate 1e-5 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--lr_scheduler_type cosine_with_min_lr \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--lr_scheduler_kwargs &quot;{\&quot;min_lr\&quot;: 0}&quot; \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--warmup_steps 40 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--weight_decay 0.0 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--max_grad_norm 1.0 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--adam_beta1 0.9 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--adam_beta2 0.95 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--per_device_train_batch_size 11 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--gradient_accumulation_steps 6 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--gradient_checkpointing \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--gradient_checkpointing_kwargs &quot;{\&quot;use_reentrant\&quot;: false}&quot; \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--num_train_epochs 3 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--use_liger \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--deepspeed ./ds-config.json \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--output_dir /tmp/Qwen2.5-3B-Infinity-Instruct-0625 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--report_to wandb \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--run_name my-second-sft-exp</span><br></span></code></pre></div></div></div></div></details>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>ds-config.json</summary><div><div class="collapsibleContent_i85q"><div class="language-json codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-json codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token property" style="color:#36acaa">&quot;fp16&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token property" style="color:#36acaa">&quot;enabled&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token boolean" style="color:#36acaa">false</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">}</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token property" style="color:#36acaa">&quot;optimizer&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token property" style="color:#36acaa">&quot;type&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;AdamW&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token property" style="color:#36acaa">&quot;params&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token property" style="color:#36acaa">&quot;lr&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;auto&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token property" style="color:#36acaa">&quot;betas&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;auto&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token property" style="color:#36acaa">&quot;eps&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;auto&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token property" style="color:#36acaa">&quot;weight_decay&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;auto&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token punctuation" style="color:#393A34">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">}</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token property" style="color:#36acaa">&quot;zero_optimization&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token property" style="color:#36acaa">&quot;stage&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">2</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token property" style="color:#36acaa">&quot;overlap_comm&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token boolean" style="color:#36acaa">false</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token property" style="color:#36acaa">&quot;allgather_bucket_size&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">5e8</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token property" style="color:#36acaa">&quot;reduce_bucket_size&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;auto&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token property" style="color:#36acaa">&quot;allgather_partitions&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token boolean" style="color:#36acaa">true</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token property" style="color:#36acaa">&quot;reduce_scatter&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token boolean" style="color:#36acaa">true</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token property" style="color:#36acaa">&quot;contiguous_gradients&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token boolean" style="color:#36acaa">true</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token property" style="color:#36acaa">&quot;round_robin_gradients&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token boolean" style="color:#36acaa">true</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">}</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token property" style="color:#36acaa">&quot;gradient_accumulation_steps&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;auto&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token property" style="color:#36acaa">&quot;gradient_clipping&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;auto&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token property" style="color:#36acaa">&quot;train_batch_size&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;auto&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token property" style="color:#36acaa">&quot;train_micro_batch_size_per_gpu&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;auto&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token property" style="color:#36acaa">&quot;wall_clock_breakdown&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token boolean" style="color:#36acaa">false</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">}</span><br></span></code></pre></div></div></div></div></details>
<p>Thanks to all the optimizations, I was able to fine-tune a 3B model instead of the 0.5B model used in the first part.</p>
<p>It did take a considerable amount of time (about 133 hours) to complete the training on V100s, so I highly recommend use modern GPUs and enabling Flash Attention and sample packing for better performance.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="evaluating">Evaluating<a href="#evaluating" class="hash-link" aria-label="EvaluatingÁöÑÁõ¥Êé•ÈìæÊé•" title="EvaluatingÁöÑÁõ¥Êé•ÈìæÊé•">‚Äã</a></h2>
<p>Now that the training is complete, it‚Äôs important to evaluate whether everything was done correctly.</p>
<p>A quick way to check the model&#x27;s performance is to interact with it. You can refer to <a href="https://huggingface.co/Qwen/Qwen2.5-3B-Instruct#quickstart" target="_blank" rel="noopener noreferrer">the quickstart section of the official SFT model</a> to try it out. Here&#x27;s an example of interacting with the model I just fine-tuned:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Me: Give me a short introduction to large language model.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">AI: A large language model (LLM) is a type of artificial intelligence (AI) that is designed to understand and generate human language. These models are trained on vast amounts of text data, allowing them to learn patterns, structures, and nuances</span><br></span></code></pre></div></div>
<p>(Note: that the response from the AI is truncated due to the <code>max_new_tokens</code> I set, but you can see that the model is responding appropriately.)</p>
<p>While direct interactions are useful for quick checks, formal evaluations are essential for more rigorous validation. Evaluating LLMs is quite a broad topic, and I&#x27;m only going to share a few tips here.</p>
<p>There are several frameworks available for evaluating LLMs, making it challenging to choose the best one, and comparing results from different frameworks can sometimes lead to unfair conclusions.</p>
<p>One well-known evaluation platform is the <a href="https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/" target="_blank" rel="noopener noreferrer">Open LLM Leaderboard</a>, which ranks LLMs based on their evaluation results. The Open LLM Leaderboard uses <a href="https://github.com/EleutherAI/lm-evaluation-harness" target="_blank" rel="noopener noreferrer">lm-evaluation-harness</a> as its backend. By using this same tool, you can ensure fair comparisons with models in the leaderboard. So for this tutorial, I&#x27;ll to use <code>lm-evaluation-harness</code> to run the same evaluations used on the Open LLM Leaderboard to assess the model I just fine-tuned.</p>
<p>The <code>lm-evaluation-harness</code> <a href="https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/tasks/leaderboard/README.md" target="_blank" rel="noopener noreferrer">integrates all the tasks used in the Open LLM Leaderboard</a>. To evaluate your model on these tasks, you can run the following command:</p>
<div class="language-sh codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-sh codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">lm_eval \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --model hf \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --model_args pretrained=$MODEL_YOU_WANT_TO_EVAL \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --tasks leaderboard</span><br></span></code></pre></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="math-hard-task-unavailable">MATH-hard Task Unavailable<a href="#math-hard-task-unavailable" class="hash-link" aria-label="MATH-hard Task UnavailableÁöÑÁõ¥Êé•ÈìæÊé•" title="MATH-hard Task UnavailableÁöÑÁõ¥Êé•ÈìæÊé•">‚Äã</a></h3>
<p>However, As of writing, the <code>competition_math</code> dataset is <a href="https://huggingface.co/datasets/hendrycks/competition_math/discussions/5" target="_blank" rel="noopener noreferrer">currently unavailable due to legal issues</a>. As a result, we&#x27;ll need to skip the <code>MATH-hard</code> task that relies on this dataset. You can modify your script to include all other tasks except <code>leaderboard_math_hard</code>:</p>
<div class="language-sh codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-sh codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">lm_eval \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --model hf \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --model_args pretrained=$MODEL_YOU_WANT_TO_EVAL \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --tasks leaderboard_bbh,leaderboard_gpqa,leaderboard_ifeval,leaderboard_mmlu_pro,leaderboard_musr</span><br></span></code></pre></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="evaluating-code-generation">Evaluating Code Generation<a href="#evaluating-code-generation" class="hash-link" aria-label="Evaluating Code GenerationÁöÑÁõ¥Êé•ÈìæÊé•" title="Evaluating Code GenerationÁöÑÁõ¥Êé•ÈìæÊé•">‚Äã</a></h3>
<p>In addition to the leaderboard evaluations, if you&#x27;re interested in evaluating your model on code generation tasks (such as <a href="https://github.com/EleutherAI/lm-evaluation-harness/tree/main/lm_eval/tasks/humaneval" target="_blank" rel="noopener noreferrer">humaneval</a>), keep in mind that the generated code usually needs to be executed to evaluate its correctness. Since executing LLM generated code can be risky, most frameworks will default to abort on such tasks. To allow code execution during evaluation, you need to set <code>HF_ALLOW_CODE_EVAL</code> to <code>1</code> and include the <code>--confirm_run_unsafe_code</code> argument in your evaluation command:</p>
<div class="language-sh codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-sh codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">lm_eval \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --model hf \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --model_args pretrained=$MODEL_YOU_WANT_TO_EVAL \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --tasks leaderboard_bbh,leaderboard_gpqa,leaderboard_ifeval,leaderboard_mmlu_pro,leaderboard_musr \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --confirm_run_unsafe_code  # Add this line</span><br></span></code></pre></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="ConclusionÁöÑÁõ¥Êé•ÈìæÊé•" title="ConclusionÁöÑÁõ¥Êé•ÈìæÊé•">‚Äã</a></h2>
<p>In this post, we‚Äôve covered everything from the basic setup to advanced techniques for scaling large language models in a single-node, multi-GPU environment. By utilizing DeepSpeed and trl, we can efficiently fine-tune models like Qwen2-3B and beyond, even on hardware that would otherwise be unable to support such models. I&#x27;ve also uploaded the fine-tuned model to the Hugging Face model hub, so you can try it out for yourself: <a href="https://huggingface.co/jlzhou/Qwen2.5-3B-Infinity-Instruct-0625" target="_blank" rel="noopener noreferrer">https://huggingface.co/jlzhou/Qwen2.5-3B-Infinity-Instruct-0625</a>.</p>
<p>In the next part of this series, we‚Äôll explore distributed training across multiple nodes, tackling more complex setups with multiple GPUs across different machines. Stay tuned!</p></div><footer class="docusaurus-mt-lg"><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><b>Ê†áÁ≠æÔºö</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/tags/llm">LLM</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/tags/distributed-training">distributed-training</a></li></ul></div></div><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><a href="https://github.com/edwardzjl/edwardzjl.github.io/blob/main/blog/2025-02-07-distributed-sft-part-2/index.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>ÁºñËæëÊ≠§È°µ</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="ÂçöÊñáÂàÜÈ°µÂØºËà™"><a class="pagination-nav__link pagination-nav__link--next" href="/distributed-sft-part-1"><div class="pagination-nav__sublabel">ËæÉÊóß‰∏ÄÁØá</div><div class="pagination-nav__label">Distributed SFT Part 1: Starting Locally</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#introduction" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#prerequisites" class="table-of-contents__link toc-highlight">Prerequisites</a></li><li><a href="#tuning-hyperparameters" class="table-of-contents__link toc-highlight">Tuning Hyperparameters</a><ul><li><a href="#gradient-accumulation" class="table-of-contents__link toc-highlight">Gradient Accumulation</a></li><li><a href="#gradient-checkpointing" class="table-of-contents__link toc-highlight">Gradient Checkpointing</a></li><li><a href="#zero" class="table-of-contents__link toc-highlight">ZeRO</a></li><li><a href="#liger-kernel" class="table-of-contents__link toc-highlight">Liger Kernel</a></li><li><a href="#sample-packing" class="table-of-contents__link toc-highlight">Sample Packing</a></li></ul></li><li><a href="#distributed-training" class="table-of-contents__link toc-highlight">Distributed Training</a></li><li><a href="#evaluating" class="table-of-contents__link toc-highlight">Evaluating</a><ul><li><a href="#math-hard-task-unavailable" class="table-of-contents__link toc-highlight">MATH-hard Task Unavailable</a></li><li><a href="#evaluating-code-generation" class="table-of-contents__link toc-highlight">Evaluating Code Generation</a></li></ul></li><li><a href="#conclusion" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></div></div></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">¬© 2026 <a href="https://github.com/edwardzjl">Junlin Zhou</a>. Licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a><img src="https://mirrors.creativecommons.org/presskit/icons/cc.svg" alt="" style="max-width: 1em;max-height:1em;margin-left: .2em;"><img src="https://mirrors.creativecommons.org/presskit/icons/by.svg" alt="" style="max-width: 1em;max-height:1em;margin-left: .2em;"><img src="https://mirrors.creativecommons.org/presskit/icons/nc.svg" alt="" style="max-width: 1em;max-height:1em;margin-left: .2em;"><img src="https://mirrors.creativecommons.org/presskit/icons/sa.svg" alt="" style="max-width: 1em;max-height:1em;margin-left: .2em;"></div></div></div></footer></div>
</body>
</html>