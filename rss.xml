<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>Edwardzjl Blog</title>
        <link>https://edwardzjl.github.io</link>
        <description>Edwardzjl Blog</description>
        <lastBuildDate>Fri, 07 Feb 2025 00:00:00 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>zh-Hans</language>
        <item>
            <title><![CDATA[Distributed SFT Part 2: Scaling Locally]]></title>
            <link>https://edwardzjl.github.io/distributed-sft-part-2</link>
            <guid>https://edwardzjl.github.io/distributed-sft-part-2</guid>
            <pubDate>Fri, 07 Feb 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[Introduction]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="introduction">Introduction<a href="https://edwardzjl.github.io/distributed-sft-part-2#introduction" class="hash-link" aria-label="Introduction的直接链接" title="Introduction的直接链接">​</a></h2>
<p><a href="https://edwardzjl.github.io/distributed-sft-part-1" target="_blank" rel="noopener noreferrer">In the first part of this series</a>, we covered the basics of setting up a local SFT experiment using <code>trl</code>. We learned how to format datasets for <code>trl</code>'s <code>SFTTrainer</code> and preprocess them to fit the required structure.</p>
<p>Now, it's time to take the next step. In this post, we'll focus on scaling the SFT setup to handle larger tasks. Specifically, we'll explore how to fine-tune an LLM in a single-node, multi-GPU environment. Along the way, we'll discuss optimization techniques to reduce memory usage, speed up training, and enable fine-tuning of even larger models. Let's get started!</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="prerequisites">Prerequisites<a href="https://edwardzjl.github.io/distributed-sft-part-2#prerequisites" class="hash-link" aria-label="Prerequisites的直接链接" title="Prerequisites的直接链接">​</a></h2>
<p>To follow along with this tutorial, you'll need a machine equipped with multiple NVIDIA GPUs. Ensure that the GPUs are connected via high-speed interconnects to minimize communication overhead. For reference, I ran this experiment using 8 NVIDIA V100 SXM2 GPUs.</p>
<p><strong>Important Considerations:</strong></p>
<ol>
<li>
<p>GPU Architecture: While I ran this experiment with V100 GPUs, newer architectures like Ampere or Hopper are strongly recommended. These GPUs offers advanced features, such as support for more efficient precision types and improved communication speeds. Additionally, techniques like <a href="https://github.com/Dao-AILab/flash-attention" target="_blank" rel="noopener noreferrer">flash-attention</a> are <a href="https://github.com/Dao-AILab/flash-attention/issues/524" target="_blank" rel="noopener noreferrer">only compatible with Ampere or newer GPUs</a>.</p>
</li>
<li>
<p>Interconnect Quality: Verify GPU communication bandwidth using <code>nvidia-smi topo -m</code>. Poor interconnects can become a bottleneck during training.</p>
</li>
</ol>
<p>Additionally, you'll need to install the following dependencies:</p>
<div class="language-txt codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-txt codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">datasets</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">torch</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">transformers</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">trl</span><br></span></code></pre></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="tuning-hyperparameters">Tuning Hyperparameters<a href="https://edwardzjl.github.io/distributed-sft-part-2#tuning-hyperparameters" class="hash-link" aria-label="Tuning Hyperparameters的直接链接" title="Tuning Hyperparameters的直接链接">​</a></h2>
<p><a href="https://huggingface.co/datasets/BAAI/Infinity-Instruct" target="_blank" rel="noopener noreferrer">BAAI/Infinity-Instruct</a> provides several officially fine-tuned models, including <a href="https://huggingface.co/BAAI/Infinity-Instruct-7M-Gen-Llama3_1-70B" target="_blank" rel="noopener noreferrer">Llama3.1-70B</a>, <a href="https://huggingface.co/BAAI/Infinity-Instruct-7M-Gen-mistral-7B" target="_blank" rel="noopener noreferrer">mistral-7B</a>, <a href="https://huggingface.co/BAAI/Infinity-Instruct-3M-0625-Qwen2-7B" target="_blank" rel="noopener noreferrer">Qwen2-7B</a> and <a href="https://huggingface.co/BAAI/Infinity-Instruct-3M-0625-Yi-1.5-9B" target="_blank" rel="noopener noreferrer">Yi-1.5-9B</a>. They also generously share the training details for these models.</p>
<p>For this tutorial, we'll use the <a href="https://huggingface.co/BAAI/Infinity-Instruct-3M-0625-Qwen2-7B#training-details" target="_blank" rel="noopener noreferrer">hyperparameters for Qwen2-7B</a> as a reference. Here's how these hyperparameters map to training arguments in <code>trl</code>:</p>
<ul>
<li>epoch: <code>--num_train_epochs</code></li>
<li>lr: <code>--learning_rate</code></li>
<li>lr_warmup_steps: <code>--warmup_steps</code></li>
<li>lr_decay_style: <code>--lr_scheduler_type</code> (Set to <code>cosine_with_min_lr</code> along with <code>min_lr</code>. Available scheduler options can be found <a href="https://huggingface.co/docs/transformers/main_classes/optimizer_schedules#transformers.SchedulerType" target="_blank" rel="noopener noreferrer">here</a>.)</li>
<li>min_lr: <code>--lr_scheduler_kwargs</code> (Set to <code>"{\"min_lr\": 0}"</code>. This argument isn't clearly documented; I discovered it through <a href="https://github.com/huggingface/transformers/pull/29341" target="_blank" rel="noopener noreferrer">this PR</a> and <a href="https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/tests/trainer/test_trainer.py#L1065" target="_blank" rel="noopener noreferrer">this test case</a>.)</li>
<li>weight_decay: <code>--weight_decay</code></li>
<li>adam_beta1: <code>--adam_beta1</code></li>
<li>adam_beta2: <code>--adam_beta2</code></li>
<li>clip_grad: <code>--max_grad_norm</code></li>
</ul>
<p>One additional parameter worth mentioning is the <code>global_batch_size</code>, which isn't directly set in the training script. The global batch size is determined by the equation:</p>
<p><code>global_batch_size = per_device_train_batch_size * gradient_accumulation_steps * numGPUs</code>.</p>
<p>For example, if our target global batch size is 528 and we're using 8 GPUs, the local batch size (per GPU) would be:</p>
<p><code>528 / 8 = 66</code>.</p>
<p>If we can fit 2 samples per batch in each GPU, we can then set <code>per_device_train_batch_size</code> to 2 and <code>gradient_accumulation_steps</code> to 33.</p>
<p>Another important consideration is training precision. Modern GPUs (Ampere series or newer) supports <code>bf16</code> and <code>tf32</code>, while older GPUs only support <code>fp16</code> and <code>fp32</code>. When fine-tuning, make sure the precision matches that of the base model. Specifically, avoid using <code>fp16</code> if the base model was trained with <code>bf16</code>. For more details, refer to <a href="https://github.com/huggingface/transformers/pull/10956" target="_blank" rel="noopener noreferrer">this PR</a>.</p>
<p>You can find the data type of your base model by the <code>torch_dtype</code> field in the <code>config.json</code> file. So if you're fine-tuning a <code>bf16</code> model but don't have access to Ampere or newer GPUs (like me), it's best to stick with <code>fp32</code> for now.</p>
<p>Now that we've covered the essential hyperparameters and considerations, let's move on to some optimization techniques that will help improve training efficiency and resource usage.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="gradient-accumulation">Gradient Accumulation<a href="https://edwardzjl.github.io/distributed-sft-part-2#gradient-accumulation" class="hash-link" aria-label="Gradient Accumulation的直接链接" title="Gradient Accumulation的直接链接">​</a></h3>
<p>You may have noticed that I used <code>per_device_train_batch_size</code> and <code>gradient_accumulation_steps</code> to calculate the local batch size. Gradient accumulation allows you to accumulate gradients over multiple mini-batches before updating the model. This technique is particularly useful when the desired batch size exceeds your hardware's memory capacity.</p>
<p>As a general guideline:</p>
<ul>
<li>Use the largest <code>per_device_train_batch_size</code> that fits within your VRAM</li>
<li>Adjust <code>gradient_accumulation_steps</code> to achieve your target batch size if necessary.</li>
</ul>
<p>This way, you can effectively simulate a larger batch size without running into memory limitations</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="gradient-checkpointing">Gradient Checkpointing<a href="https://edwardzjl.github.io/distributed-sft-part-2#gradient-checkpointing" class="hash-link" aria-label="Gradient Checkpointing的直接链接" title="Gradient Checkpointing的直接链接">​</a></h3>
<p>Gradient checkpointing is a memory optimization technique that reduces memory usage by trading off computation. During training, a large portion of memory is used to store intermediate activations for backpropagation. Gradient checkpointing reduces this memory usage by selectively saving a subset of activations and recomputing the rest during the backward pass.</p>
<p>Note: According to <a href="https://pytorch.org/docs/stable/checkpoint.html" target="_blank" rel="noopener noreferrer">https://pytorch.org/docs/stable/checkpoint.html</a>:</p>
<blockquote>
<p>There are currently two checkpointing implementations available, determined by the <code>use_reentrant</code> parameter. It is recommended that you use <code>use_reentrant=False</code>.</p>
</blockquote>
<p>You can read that section for a deeper understanding of  the differences between the two implementations.</p>
<p>At the time of writing, the <code>transformers</code> library (v4.48.1) <a href="https://github.com/huggingface/transformers/blob/2e752ead46a8845e8a160d2043c1336447895690/src/transformers/modeling_utils.py#L2538" target="_blank" rel="noopener noreferrer">uses the reentrant implementation by default</a>. To use the non-reentrant version, you must explicitly pass the following argument:</p>
<div class="language-sh codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-sh codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">--gradient_checkpointing_kwargs "{\"use_reentrant\": false}"</span><br></span></code></pre></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="zero">ZeRO<a href="https://edwardzjl.github.io/distributed-sft-part-2#zero" class="hash-link" aria-label="ZeRO的直接链接" title="ZeRO的直接链接">​</a></h3>
<p>There are several approaches to parallelizing training tasks, including <strong>Data Parallelism (DP)</strong>, <strong>Tensor Parallelism (TP)</strong>, <strong>Pipeline Parallelism (PP)</strong>, <strong>Zero Redundancy Optimizer (ZeRO)</strong>, <strong>Sequence Parallelism</strong> and <strong>Expert Parallelism</strong>. For a detailed overview of these methods, I recommend checking out <a href="https://github.com/stas00/ml-engineering/tree/master/training/model-parallelism#scalability-concepts" target="_blank" rel="noopener noreferrer">this excellent resource</a>.</p>
<p>In this tutorial, we'll focus on <strong>ZeRO</strong>, which provides greater efficiency than traditional DP without requiring modifications to the training code.</p>
<p>ZeRO (Zero Redundancy Optimizer) is a powerful technique for scaling training by reducing memory usage. If you're new to ZeRO, check out <a href="https://arxiv.org/abs/1910.02054" target="_blank" rel="noopener noreferrer">the original paper</a> or <a href="https://github.com/stas00/ml-engineering/tree/master/training/model-parallelism#zero-data-parallelism" target="_blank" rel="noopener noreferrer">this detailed article</a>.</p>
<p>ZeRO has three stages, each targeting different aspects  of memory savings:</p>
<ul>
<li>Stage 1: Reduces optimizer state memory.</li>
<li>Stage 2: Further reduces gradient memory.</li>
<li>Stage 3: Fully partitions model states, achieving the highest memory savings at the cost of increased communication overhead.</li>
</ul>
<p>Stage 3 provides the greatest memory efficiency but can significantly slow down training if inter-GPU communication is not fast enough. As a general guideline:</p>
<ul>
<li>Start with Stage 2.</li>
<li>Try Stage 3 only if Stage 2 still leads to CUDA OOM.</li>
</ul>
<p>That being said, it’s always worth testing both Stage 2 and Stage 3 on your setup to determine which one performs better on your hardware.</p>
<p>For this tutorial, we will use the official implementation of ZeRO -- <a href="https://www.deepspeed.ai/" target="_blank" rel="noopener noreferrer">DeepSpeed</a>. To use DeepSpeed, you'll need to install it first. DeepSpeed provides C++/CUDA ops that can be pre-installed or JIT-compiled. If you choose the pre-installation option, refer to <a href="https://www.deepspeed.ai/tutorials/advanced-install/#pre-install-deepspeed-ops" target="_blank" rel="noopener noreferrer">this documentation</a>. we’ll install DeepSpeed using the JIT method by running:</p>
<div class="language-sh codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-sh codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip install deepspeed</span><br></span></code></pre></div></div>
<p>The Hugging Face <code>transformers</code> library provides built-in support for DeepSpeed. You can enable it by specifying a DeepSpeed config file using the <code>--deepspeed</code> flag in your training script. For more information, refer to the <a href="https://huggingface.co/docs/transformers/deepspeed" target="_blank" rel="noopener noreferrer">DeepSpeed documentation in transformers</a>.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="liger-kernel">Liger Kernel<a href="https://edwardzjl.github.io/distributed-sft-part-2#liger-kernel" class="hash-link" aria-label="Liger Kernel的直接链接" title="Liger Kernel的直接链接">​</a></h3>
<p><a href="https://github.com/linkedin/Liger-Kernel" target="_blank" rel="noopener noreferrer">Liger Kernel</a> is a collection of Triton kernels developed by LinkedIn to reduce memory usage and increase training throughput. The best part is that it requires no complex configuration, making it an easy addition to your setup. To install it, run:</p>
<div class="language-sh codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-sh codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip install liger-kernel</span><br></span></code></pre></div></div>
<p>Once installed, add the <code>--use_liger</code> flag to your training script, and you'll automatically save VRAM without any extra setup or hassle. It's a straightforward way to optimize your training without sacrificing performance.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="sample-packing">Sample Packing<a href="https://edwardzjl.github.io/distributed-sft-part-2#sample-packing" class="hash-link" aria-label="Sample Packing的直接链接" title="Sample Packing的直接链接">​</a></h3>
<p>Large models are trained on GPUs to leverage their parallelism. However, in the context of language models, where we train on text sequences, the length of each sample varies.</p>
<p>The traditional approach to handle variable-length sequences is to pad each sample to match the longest one in a batch. While this ensures uniform input dimensions, it also leads to considerable memory waste due to the padding.</p>
<p><a href="https://arxiv.org/abs/2407.09105" target="_blank" rel="noopener noreferrer">Sample packing</a> addresses this issue by combining shorter samples into a single sequence. This technique allows for more efficient GPU memory usage, reducing waste and potentially speeding up training.</p>
<p>While the concept is straightforward, Implementing it correctly can be one of the most challenging tasks of this experiment for me.</p>
<p>At the first glance, <a href="https://huggingface.co/docs/trl/sft_trainer#packing-dataset--constantlengthdataset-" target="_blank" rel="noopener noreferrer">trl supports packing dataset by simply passing an argument</a>. However, uppon <a href="https://github.com/huggingface/trl/blob/f34b70a32ef2820d3fd5c5b1ff6d1fd1e7799f04/trl/trainer/sft_trainer.py#L459" target="_blank" rel="noopener noreferrer">further investigation</a>, I found that the implementation might not suit my needs. As pointed out in <a href="https://github.com/huggingface/trl/issues/805" target="_blank" rel="noopener noreferrer">this issue</a>, the attention mask is not handled properly, which can lead to potential cross contamination in attention between sequences. The following image illustrates this issue clearly. On the left is the result of using <code>--packing</code>, and on the right is the correct way to pack samples:</p>
<p><img decoding="async" loading="lazy" src="https://cdn-uploads.huggingface.co/production/uploads/63eb008e5c837d9968f1eb71/lzpKqOADV5mdOdclPbQ9C.png" alt="image/png" class="img_ev3q"></p>
<p>After further digging, I found that, at least for now, <a href="https://github.com/huggingface/transformers/issues/27640#issuecomment-2619471784" target="_blank" rel="noopener noreferrer">'the correct way of packing' is supported only with Flash Attention</a>. If you don't have access to Ampere or newer GPUs, you may need to stick with the traditional padding approach.</p>
<p>However, if you're lucky enough to have those GPUs, you can follow <a href="https://huggingface.co/blog/packing-with-FA2" target="_blank" rel="noopener noreferrer">this blog post</a> to enable sample packing during training. Note that I haven't personally validated this approach. Also, as of writing, there are some PRs related to this feature that aren't released yet (for example <a href="https://github.com/huggingface/trl/pull/2158" target="_blank" rel="noopener noreferrer">this one</a>). To access this functionality, you may need to install <code>trl</code> and <code>transformers</code> from source:</p>
<div class="language-sh codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-sh codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip install git+https://github.com/huggingface/trl</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">pip install git+https://github.com/huggingface/tranformers</span><br></span></code></pre></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="distributed-training">Distributed Training<a href="https://edwardzjl.github.io/distributed-sft-part-2#distributed-training" class="hash-link" aria-label="Distributed Training的直接链接" title="Distributed Training的直接链接">​</a></h2>
<p>With all the optimizations in place, we're now ready to scale our SFT experiment across multiple GPUs. To do so, we can use tools like <a href="https://pytorch.org/docs/stable/elastic/run.html" target="_blank" rel="noopener noreferrer">torchrun</a>, <a href="https://www.deepspeed.ai/getting-started/" target="_blank" rel="noopener noreferrer">deepspeed</a> or <a href="https://huggingface.co/docs/accelerate/index" target="_blank" rel="noopener noreferrer">accelerate</a>. Personally I prefer <code>torchrun</code> for its simplicity and ease of use.</p>
<p>By running the following command, we can distribute the training job across multiple GPUs:</p>
<p>Oh, and don't forget to set up <code>wandb</code> for logging — we're doing proper fine-tuning now! 😉</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>sft2.sh</summary><div><div class="collapsibleContent_i85q"><div class="language-sh codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-sh codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">torchrun \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--nproc_per_node 8 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">sft.py \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--model_name_or_path Qwen/Qwen2.5-3B \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--dataset_name BAAI/Infinity-Instruct \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--dataset_config 0625 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--do_train \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--learning_rate 1e-5 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--lr_scheduler_type cosine_with_min_lr \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--lr_scheduler_kwargs "{\"min_lr\": 0}" \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--warmup_steps 40 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--weight_decay 0.0 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--max_grad_norm 1.0 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--adam_beta1 0.9 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--adam_beta2 0.95 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--per_device_train_batch_size 11 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--gradient_accumulation_steps 6 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--gradient_checkpointing \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--gradient_checkpointing_kwargs "{\"use_reentrant\": false}" \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--num_train_epochs 3 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--use_liger \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--deepspeed ./ds-config.json \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--output_dir /tmp/Qwen2.5-3B-Infinity-Instruct-0625 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--report_to wandb \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--run_name my-second-sft-exp</span><br></span></code></pre></div></div></div></div></details>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>ds-config.json</summary><div><div class="collapsibleContent_i85q"><div class="language-json codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-json codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token property" style="color:#36acaa">"fp16"</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token property" style="color:#36acaa">"enabled"</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token boolean" style="color:#36acaa">false</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">}</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token property" style="color:#36acaa">"optimizer"</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token property" style="color:#36acaa">"type"</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">"AdamW"</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token property" style="color:#36acaa">"params"</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token property" style="color:#36acaa">"lr"</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">"auto"</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token property" style="color:#36acaa">"betas"</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">"auto"</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token property" style="color:#36acaa">"eps"</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">"auto"</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token property" style="color:#36acaa">"weight_decay"</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">"auto"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token punctuation" style="color:#393A34">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">}</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token property" style="color:#36acaa">"zero_optimization"</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token property" style="color:#36acaa">"stage"</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">2</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token property" style="color:#36acaa">"overlap_comm"</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token boolean" style="color:#36acaa">false</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token property" style="color:#36acaa">"allgather_bucket_size"</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">5e8</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token property" style="color:#36acaa">"reduce_bucket_size"</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">"auto"</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token property" style="color:#36acaa">"allgather_partitions"</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token boolean" style="color:#36acaa">true</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token property" style="color:#36acaa">"reduce_scatter"</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token boolean" style="color:#36acaa">true</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token property" style="color:#36acaa">"contiguous_gradients"</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token boolean" style="color:#36acaa">true</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token property" style="color:#36acaa">"round_robin_gradients"</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token boolean" style="color:#36acaa">true</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">}</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token property" style="color:#36acaa">"gradient_accumulation_steps"</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">"auto"</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token property" style="color:#36acaa">"gradient_clipping"</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">"auto"</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token property" style="color:#36acaa">"train_batch_size"</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">"auto"</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token property" style="color:#36acaa">"train_micro_batch_size_per_gpu"</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">"auto"</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token property" style="color:#36acaa">"wall_clock_breakdown"</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token boolean" style="color:#36acaa">false</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">}</span><br></span></code></pre></div></div></div></div></details>
<p>Thanks to all the optimizations, I was able to fine-tune a 3B model instead of the 0.5B model used in the first part.</p>
<p>It did take a considerable amount of time (about 133 hours) to complete the training on V100s, so I highly recommend use modern GPUs and enabling Flash Attention and sample packing for better performance.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="evaluating">Evaluating<a href="https://edwardzjl.github.io/distributed-sft-part-2#evaluating" class="hash-link" aria-label="Evaluating的直接链接" title="Evaluating的直接链接">​</a></h2>
<p>Now that the training is complete, it’s important to evaluate whether everything was done correctly.</p>
<p>A quick way to check the model's performance is to interact with it. You can refer to <a href="https://huggingface.co/Qwen/Qwen2.5-3B-Instruct#quickstart" target="_blank" rel="noopener noreferrer">the quickstart section of the official SFT model</a> to try it out. Here's an example of interacting with the model I just fine-tuned:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Me: Give me a short introduction to large language model.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">AI: A large language model (LLM) is a type of artificial intelligence (AI) that is designed to understand and generate human language. These models are trained on vast amounts of text data, allowing them to learn patterns, structures, and nuances</span><br></span></code></pre></div></div>
<p>(Note: that the response from the AI is truncated due to the <code>max_new_tokens</code> I set, but you can see that the model is responding appropriately.)</p>
<p>While direct interactions are useful for quick checks, formal evaluations are essential for more rigorous validation. Evaluating LLMs is quite a broad topic, and I'm only going to share a few tips here.</p>
<p>There are several frameworks available for evaluating LLMs, making it challenging to choose the best one, and comparing results from different frameworks can sometimes lead to unfair conclusions.</p>
<p>One well-known evaluation platform is the <a href="https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/" target="_blank" rel="noopener noreferrer">Open LLM Leaderboard</a>, which ranks LLMs based on their evaluation results. The Open LLM Leaderboard uses <a href="https://github.com/EleutherAI/lm-evaluation-harness" target="_blank" rel="noopener noreferrer">lm-evaluation-harness</a> as its backend. By using this same tool, you can ensure fair comparisons with models in the leaderboard. So for this tutorial, I'll to use <code>lm-evaluation-harness</code> to run the same evaluations used on the Open LLM Leaderboard to assess the model I just fine-tuned.</p>
<p>The <code>lm-evaluation-harness</code> <a href="https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/tasks/leaderboard/README.md" target="_blank" rel="noopener noreferrer">integrates all the tasks used in the Open LLM Leaderboard</a>. To evaluate your model on these tasks, you can run the following command:</p>
<div class="language-sh codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-sh codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">lm_eval \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --model hf \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --model_args pretrained=$MODEL_YOU_WANT_TO_EVAL \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --tasks leaderboard</span><br></span></code></pre></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="math-hard-task-unavailable">MATH-hard Task Unavailable<a href="https://edwardzjl.github.io/distributed-sft-part-2#math-hard-task-unavailable" class="hash-link" aria-label="MATH-hard Task Unavailable的直接链接" title="MATH-hard Task Unavailable的直接链接">​</a></h3>
<p>However, As of writing, the <code>competition_math</code> dataset is <a href="https://huggingface.co/datasets/hendrycks/competition_math/discussions/5" target="_blank" rel="noopener noreferrer">currently unavailable due to legal issues</a>. As a result, we'll need to skip the <code>MATH-hard</code> task that relies on this dataset. You can modify your script to include all other tasks except <code>leaderboard_math_hard</code>:</p>
<div class="language-sh codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-sh codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">lm_eval \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --model hf \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --model_args pretrained=$MODEL_YOU_WANT_TO_EVAL \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --tasks leaderboard_bbh,leaderboard_gpqa,leaderboard_ifeval,leaderboard_mmlu_pro,leaderboard_musr</span><br></span></code></pre></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="evaluating-code-generation">Evaluating Code Generation<a href="https://edwardzjl.github.io/distributed-sft-part-2#evaluating-code-generation" class="hash-link" aria-label="Evaluating Code Generation的直接链接" title="Evaluating Code Generation的直接链接">​</a></h3>
<p>In addition to the leaderboard evaluations, if you're interested in evaluating your model on code generation tasks (such as <a href="https://github.com/EleutherAI/lm-evaluation-harness/tree/main/lm_eval/tasks/humaneval" target="_blank" rel="noopener noreferrer">humaneval</a>), keep in mind that the generated code usually needs to be executed to evaluate its correctness. Since executing LLM generated code can be risky, most frameworks will default to abort on such tasks. To allow code execution during evaluation, you need to set <code>HF_ALLOW_CODE_EVAL</code> to <code>1</code> and include the <code>--confirm_run_unsafe_code</code> argument in your evaluation command:</p>
<div class="language-sh codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-sh codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">lm_eval \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --model hf \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --model_args pretrained=$MODEL_YOU_WANT_TO_EVAL \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --tasks leaderboard_bbh,leaderboard_gpqa,leaderboard_ifeval,leaderboard_mmlu_pro,leaderboard_musr \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --confirm_run_unsafe_code  # Add this line</span><br></span></code></pre></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="https://edwardzjl.github.io/distributed-sft-part-2#conclusion" class="hash-link" aria-label="Conclusion的直接链接" title="Conclusion的直接链接">​</a></h2>
<p>In this post, we’ve covered everything from the basic setup to advanced techniques for scaling large language models in a single-node, multi-GPU environment. By utilizing DeepSpeed and trl, we can efficiently fine-tune models like Qwen2-3B and beyond, even on hardware that would otherwise be unable to support such models. I've also uploaded the fine-tuned model to the Hugging Face model hub, so you can try it out for yourself: <a href="https://huggingface.co/jlzhou/Qwen2.5-3B-Infinity-Instruct-0625" target="_blank" rel="noopener noreferrer">https://huggingface.co/jlzhou/Qwen2.5-3B-Infinity-Instruct-0625</a>.</p>
<p>In the next part of this series, we’ll explore distributed training across multiple nodes, tackling more complex setups with multiple GPUs across different machines. Stay tuned!</p>]]></content:encoded>
            <category>LLM</category>
            <category>distributed-training</category>
        </item>
        <item>
            <title><![CDATA[Distributed SFT Part 1: Starting Locally]]></title>
            <link>https://edwardzjl.github.io/distributed-sft-part-1</link>
            <guid>https://edwardzjl.github.io/distributed-sft-part-1</guid>
            <pubDate>Thu, 23 Jan 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[Introduction]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="introduction">Introduction<a href="https://edwardzjl.github.io/distributed-sft-part-1#introduction" class="hash-link" aria-label="Introduction的直接链接" title="Introduction的直接链接">​</a></h2>
<p>Welcome to this series of articles documenting the lessons I learned during my first attempt at running distributed supervised fine-tuning (SFT) tasks using <a href="https://github.com/huggingface/trl" target="_blank" rel="noopener noreferrer">trl</a> and <a href="https://github.com/microsoft/DeepSpeed" target="_blank" rel="noopener noreferrer">DeepSpeed</a>.</p>
<p>This series will walk you through my journey, starting with a simple local experiment and progressively scaling up to a distributed environment. The three parts of this series are:</p>
<ul>
<li>
<p><strong>Part 1: The Local Experiment</strong> -- I will show you how I ran my very first local SFT experiment, following the official <a href="https://huggingface.co/docs/trl/sft_trainer" target="_blank" rel="noopener noreferrer">trl documentation</a>.</p>
</li>
<li>
<p><strong>Part 2: Multi GPU</strong> -- We will leverage <strong>single-machine, multi-GPU</strong> parallel training to complete a full SFT task in our local environment.</p>
</li>
<li>
<p><strong>Part 3: Multi Machine</strong> -- We'll take things a step further by submitting the same training task to a Kubernetes cluster, utilizing <strong>multi-machine, multi-GPU</strong> training with <a href="https://github.com/kubeflow/training-operator" target="_blank" rel="noopener noreferrer">Kubeflow's Training Operator</a>.</p>
</li>
</ul>
<p>A quick note about myself: I'm a software development engineer who is fairly new to the field of deep learning. If these articles seem too basic for you, I appreciate your patience as I navigate this learning journey.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="prerequisites">Prerequisites<a href="https://edwardzjl.github.io/distributed-sft-part-1#prerequisites" class="hash-link" aria-label="Prerequisites的直接链接" title="Prerequisites的直接链接">​</a></h2>
<p>To follow this tutorial, you'll need a machine with at least one NVIDIA GPU. I ran the experiment on a V100 without encountering any memory issues. If your GPU has less than 32GB of VRAM, you may need to reduce the <code>per_device_train_batch_size</code> or consider using truncation (although this is not recommended) to prevent CUDA out-of-memory (OOM) errors.</p>
<p>You'll also need the following dependencies:</p>
<div class="language-txt codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-txt codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">datasets</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">transformers</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">trl</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">torch</span><br></span></code></pre></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="training">Training<a href="https://edwardzjl.github.io/distributed-sft-part-1#training" class="hash-link" aria-label="Training的直接链接" title="Training的直接链接">​</a></h2>
<p>The <code>trl</code> library offers some excellent example training scripts, and we'll start with this one: <a href="https://github.com/huggingface/trl/blob/main/trl/scripts/sft.py" target="_blank" rel="noopener noreferrer">https://github.com/huggingface/trl/blob/main/trl/scripts/sft.py</a></p>
<p>Copy the script to your development machine (or notebook), select a base model, and pick an SFT dataset to run the experiment. For this experiment, I chose <a href="https://huggingface.co/Qwen/Qwen2.5-0.5B" target="_blank" rel="noopener noreferrer">Qwen/Qwen2.5-0.5B</a> as the base model for its compact size, and <a href="https://huggingface.co/datasets/BAAI/Infinity-Instruct" target="_blank" rel="noopener noreferrer">BAAI/Infinity-Instruct</a> as the SFT dataset (somehow randomly 😌). You can explore other interesting datasets here: <a href="https://github.com/mlabonne/llm-datasets" target="_blank" rel="noopener noreferrer">https://github.com/mlabonne/llm-datasets</a>.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="command-line-arguments">Command-line Arguments<a href="https://edwardzjl.github.io/distributed-sft-part-1#command-line-arguments" class="hash-link" aria-label="Command-line Arguments的直接链接" title="Command-line Arguments的直接链接">​</a></h3>
<p>The training script (<code>sft.py</code>) exposes a variety of useful command-line arguments that allow you to customize the fine-tuning process. These arguments are mapped to specific properties in the following classes:</p>
<ul>
<li><a href="https://huggingface.co/docs/trl/v0.13.0/en/script_utils#trl.ScriptArguments" target="_blank" rel="noopener noreferrer">ScriptArguments</a></li>
<li><a href="https://github.com/huggingface/trl/blob/v0.13.0/trl/trainer/model_config.py#L20" target="_blank" rel="noopener noreferrer">ModelConfig</a></li>
<li><a href="https://huggingface.co/docs/trl/v0.13.0/en/sft_trainer#trl.SFTConfig" target="_blank" rel="noopener noreferrer">SFTConfig</a>, which extends <a href="https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments" target="_blank" rel="noopener noreferrer">TrainingArguments</a></li>
</ul>
<p>You can pass any of these arguments directly from the command line by prepending them with <code>--</code>. For instance, passing <code>--dataset_name</code> will set the <code>dataset_name</code> field in the <code>trl.ScriptArguments</code> class.</p>
<p>Let's take a look at the arguments used for this tutorial:</p>
<ul>
<li><code>--model_name_or_path</code>: Specifies the base model to fine-tune.</li>
<li><code>--dataset_name</code>: Defines the dataset to use for fine-tuning.</li>
<li><code>--dataset_config</code>: Some datasets come with multiple configurations (versions). This argument lets you choose the version you want to use.</li>
<li><code>--do_train</code>: Tells the script to start the training process.</li>
<li><code>--per_device_train_batch_size</code>: Defines the batch size for each GPU.</li>
<li><code>--output_dir</code>: Specifies the directory where the model will be saved.</li>
<li><code>--max_steps</code>: Sets the maximum number of training steps.</li>
<li><code>--logging_steps</code>: Sets how often logs are recorded during training.</li>
</ul>
<p>For convenience, I prefer to save the full command in a shell script for easy execution. Here's the script I used for this tutorial:</p>
<div class="language-sh codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-sh codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">python sft.py \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --model_name_or_path Qwen/Qwen2.5-0.5B \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --dataset_name BAAI/Infinity-Instruct \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --dataset_config 0625 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --do_train \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --per_device_train_batch_size 4 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --output_dir /tmp/my-first-sft-exp \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --max_steps 10 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --logging_steps 1</span><br></span></code></pre></div></div>
<p>Notes:</p>
<ul>
<li>I selected the smallest version of the dataset and limited the experiment to just 10 steps for a quicker run.</li>
<li>Since the training is only 10 steps, I set <code>--logging_steps</code> to 1 to see logs more frequently.</li>
<li>The <code>--per_device_train_batch_size</code> is set to 4, as the goal here isn't model quality but simply to run the experiment without CUDA OOM. Any number that can fit in your VRAM should work.</li>
</ul>
<blockquote>
<p><em>Updated 2025-02-18:</em></p>
<p><code>trl</code> provides a convenient helper function to parse training args from a YAML file, you can find more details <a href="https://huggingface.co/docs/trl/main/script_utils#trl.TrlParser.parse_args_and_config" target="_blank" rel="noopener noreferrer">here</a>.</p>
<p>With this feature, you can save the above training arguments in a YAML file (e.g., <code>recipe.yaml</code>) as follows:</p>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token key atrule" style="color:#00a4db">model_name_or_path</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> Qwen/Qwen2.5</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">0.5B</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">dataset_name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> BAAI/Infinity</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">Instruct</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">dataset_config</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">'0625'</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">do_train</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token boolean important" style="color:#36acaa">true</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">per_device_train_batch_size</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">4</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">output_dir</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> /tmp/my</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">first</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">sft</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">exp</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">max_steps</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">10</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">logging_steps</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1</span><br></span></code></pre></div></div>
<p>And launch the training with:</p>
<div class="language-sh codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-sh codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">python sft.py --config recipe.yaml</span><br></span></code></pre></div></div>
</blockquote>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="the-oops">The Oops<a href="https://edwardzjl.github.io/distributed-sft-part-1#the-oops" class="hash-link" aria-label="The Oops的直接链接" title="The Oops的直接链接">​</a></h3>
<p>Now if you use the same dataset and execute the same script, you'll likely encounter a (not so helpful) error message:</p>
<div class="language-console codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-console codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">$ ./quickstart.sh </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Resolving data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:00&lt;00:00, 50.35it/s]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Map:   0%|                                                                                                                                                                         | 0/659808 [00:00&lt;?, ? examples/s]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Traceback (most recent call last):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  File "/home/jovyan/sft-walkthrough/sft.py", line 126, in &lt;module&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    main(script_args, training_args, model_args)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  File "/home/jovyan/sft-walkthrough/sft.py", line 97, in main</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    trainer = SFTTrainer(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  ...</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  File "/home/jovyan/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 416, in tokenize</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    element[dataset_text_field] if formatting_func is None else formatting_func(element),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  File "/home/jovyan/.local/lib/python3.10/site-packages/datasets/formatting/formatting.py", line 277, in __getitem__</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    value = self.data[key]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">KeyError: 'text'</span><br></span></code></pre></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="the-fix">The Fix<a href="https://edwardzjl.github.io/distributed-sft-part-1#the-fix" class="hash-link" aria-label="The Fix的直接链接" title="The Fix的直接链接">​</a></h3>
<blockquote>
<p><em>Updated 2025-02-18:</em></p>
<ul>
<li>Starting from trl 0.15.0 (in <a href="https://github.com/huggingface/trl/pull/2405" target="_blank" rel="noopener noreferrer">this PR</a>) the 'conversations' column is no longer supported. We need to rename it to 'messages'.</li>
<li>In <a href="https://github.com/huggingface/trl/pull/2862" target="_blank" rel="noopener noreferrer">this PR</a> (not yet released as of writing), support for the 'conversations' column is back and the whole preprocessing is simplified, we no longer need to map the dict keys('from' -&gt; 'role', 'value' -&gt; 'content') ourselves.</li>
</ul>
<p><em>Updated 2025-02-19:</em></p>
<p>The above PR is released in trl 0.15.1.</p>
</blockquote>
<p>This error message is a bit confusing--it states that the <code>SFTTrainer</code> requires the dataset to have a 'text' field. However, according to the <a href="https://huggingface.co/docs/trl/dataset_formats#overview-of-the-dataset-formats-and-types" target="_blank" rel="noopener noreferrer">dataset format and types</a>, 'text' is used for standard dataset, while 'messages' should be used for conversational datasets. After a lot of googling, I came across <a href="https://github.com/huggingface/trl/issues/2071" target="_blank" rel="noopener noreferrer">this tracking issue</a>, <a href="https://github.com/huggingface/trl/blob/v0.13.0/trl/trainer/sft_trainer.py#L250" target="_blank" rel="noopener noreferrer">this line of code</a> and <a href="https://github.com/huggingface/trl/blob/v0.13.0/trl/extras/dataset_formatting.py#L78" target="_blank" rel="noopener noreferrer">this function</a>. It seems that for the current implementation (<code>trl == 0.13.0</code>) we have two options:</p>
<ol>
<li>Format the dataset ourselves (apply a chat template) and place the formatted data into the 'text' field.</li>
<li>Convert our dataset in a way that allows <code>trl</code> to handle the transformation for us.</li>
</ol>
<p>For the second option to work, the dataset must:</p>
<ul>
<li>Contain a 'messages' or 'conversations' field.</li>
<li>Have each element in the 'messages' (or 'conversations') field include both a 'content' field and a 'role' field.</li>
</ul>
<p>Examining the dataset I used revealed a mismatch: while it has a 'conversations' field, the elements inside use 'from' and 'value' as keys instead of 'role' and 'content'. As a lazy coder, I opted for the second approach and updated the training script accordingly. Additionally, I also remove all other fields in the dataset, as they are unused for the SFT task. Removing them will slightly reduce memory footprint and speed up processing.</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token punctuation" style="color:#393A34">.</span><span class="token punctuation" style="color:#393A34">.</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">main</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">script_args</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> training_args</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> model_args</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">.</span><span class="token punctuation" style="color:#393A34">.</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic">################</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># Dataset</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic">################</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    dataset </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> load_dataset</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">script_args</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">dataset_name</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> name</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">script_args</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">dataset_config</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">convert_fields</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">message</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token builtin">dict</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">-</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> </span><span class="token builtin">dict</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        _message </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          </span><span class="token string" style="color:#e3116c">"role"</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> message</span><span class="token punctuation" style="color:#393A34">[</span><span class="token string" style="color:#e3116c">"from"</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          </span><span class="token string" style="color:#e3116c">"content"</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> message</span><span class="token punctuation" style="color:#393A34">[</span><span class="token string" style="color:#e3116c">"value"</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token punctuation" style="color:#393A34">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Qwen2.5 tokenizer only supports "system", "user", "assistant" and "tool"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># See &lt;https://huggingface.co/Qwen/Qwen2.5-3B/blob/main/tokenizer_config.json#L198&gt;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">if</span><span class="token plain"> _message</span><span class="token punctuation" style="color:#393A34">[</span><span class="token string" style="color:#e3116c">"role"</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">==</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">"human"</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            _message</span><span class="token punctuation" style="color:#393A34">[</span><span class="token string" style="color:#e3116c">"role"</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">"user"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">elif</span><span class="token plain"> _message</span><span class="token punctuation" style="color:#393A34">[</span><span class="token string" style="color:#e3116c">"role"</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">==</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">"gpt"</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            _message</span><span class="token punctuation" style="color:#393A34">[</span><span class="token string" style="color:#e3116c">"role"</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">"assistant"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">elif</span><span class="token plain"> _message</span><span class="token punctuation" style="color:#393A34">[</span><span class="token string" style="color:#e3116c">"role"</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">==</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">"system"</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token comment" style="color:#999988;font-style:italic"># nothing to be done.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token punctuation" style="color:#393A34">.</span><span class="token punctuation" style="color:#393A34">.</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">else</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token comment" style="color:#999988;font-style:italic"># In case there are any other roles, print them so we can improve in next iteration.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token keyword" style="color:#00009f">print</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">_message</span><span class="token punctuation" style="color:#393A34">[</span><span class="token string" style="color:#e3116c">"role"</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">return</span><span class="token plain"> _message</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">convert_messages</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">example</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        example</span><span class="token punctuation" style="color:#393A34">[</span><span class="token string" style="color:#e3116c">"conversations"</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">convert_fields</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">message</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> message </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> example</span><span class="token punctuation" style="color:#393A34">[</span><span class="token string" style="color:#e3116c">"conversations"</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">return</span><span class="token plain"> example</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># remove unused fields</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    dataset </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> dataset</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">remove_columns</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">[</span><span class="token string" style="color:#e3116c">"id"</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">"label"</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">"langdetect"</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">"source"</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token builtin">map</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">convert_messages</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">.</span><span class="token punctuation" style="color:#393A34">.</span><span class="token punctuation" style="color:#393A34">.</span><br></span></code></pre></div></div>
<p>With that update, the script ran without any issues! You should be able to see the training log like:</p>
<div class="language-console codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-console codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">$ ./quickstart.sh </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Resolving data files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:02&lt;00:00, 17.26it/s]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 659808/659808 [01:19&lt;00:00, 8280.44 examples/s]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 659808/659808 [08:33&lt;00:00, 1284.45 examples/s]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">{'loss': 1.8859, 'grad_norm': 14.986075401306152, 'learning_rate': 1.8e-05, 'epoch': 0.0}                                                                                                                                     </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">{'loss': 1.4527, 'grad_norm': 13.9092378616333, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.0}                                                                                                                        </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">{'loss': 1.467, 'grad_norm': 7.388503074645996, 'learning_rate': 1.4e-05, 'epoch': 0.0}                                                                                                                                       </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">{'loss': 1.7757, 'grad_norm': 9.457520484924316, 'learning_rate': 1.2e-05, 'epoch': 0.0}                                                                                                                                      </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">{'loss': 1.9043, 'grad_norm': 10.256357192993164, 'learning_rate': 1e-05, 'epoch': 0.0}                                                                                                                                       </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">{'loss': 1.6163, 'grad_norm': 10.774249076843262, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.0}                                                                                                                       </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">{'loss': 1.1774, 'grad_norm': 5.897563457489014, 'learning_rate': 6e-06, 'epoch': 0.0}                                                                                                                                        </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">{'loss': 1.8093, 'grad_norm': 8.3130464553833, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.0}                                                                                                                          </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">{'loss': 1.8387, 'grad_norm': 7.102719306945801, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}                                                                                                                       </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">{'loss': 1.4251, 'grad_norm': 9.853829383850098, 'learning_rate': 0.0, 'epoch': 0.0}                                                                                                                                          </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">{'train_runtime': 38.8598, 'train_samples_per_second': 1.029, 'train_steps_per_second': 0.257, 'train_loss': 1.635251808166504, 'epoch': 0.0}                                                                                 </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:38&lt;00:00,  3.89s/it]</span><br></span></code></pre></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="https://edwardzjl.github.io/distributed-sft-part-1#conclusion" class="hash-link" aria-label="Conclusion的直接链接" title="Conclusion的直接链接">​</a></h2>
<p>In this first part, we've walked through setting up a local SFT experiment using <code>trl</code>. This library provides a user-friendly interface for fine-tuning LLMs with custom datasets. We also covered the correct dataset format required for <code>trl</code>'s <code>SFTTrainer</code> and how to preprocess datasets to meet these requirements.</p>
<p>In the next part, we'll delve into scaling this setup locally using a single-node, multi-GPU configuration to tackle a complete SFT task. Additionally, we'll explore various optimization techniques to fit a bigger model into your GPU and accelerate the training process. Stay tuned!</p>]]></content:encoded>
            <category>LLM</category>
            <category>distributed-training</category>
        </item>
        <item>
            <title><![CDATA[[译] JSON格式作为配置文件的缺点]]></title>
            <link>https://edwardzjl.github.io/the-downsides-of-json-for-config-files</link>
            <guid>https://edwardzjl.github.io/the-downsides-of-json-for-config-files</guid>
            <pubDate>Fri, 09 Aug 2019 00:00:00 GMT</pubDate>
            <description><![CDATA[翻译自这篇文章]]></description>
            <content:encoded><![CDATA[<blockquote>
<p>翻译自<a href="https://www.arp242.net/json-config.html" target="_blank" rel="noopener noreferrer" title="The downsides of JSON for config files">这篇文章</a></p>
</blockquote>
<p>我最近接触到许多项目将 <code>JSON</code> 用作配置文件。我认为这不是一个好主意。</p>
<p><code>JSON</code> 从设计之初就不是用于做配置文件的，这也不是它擅长的领域。<code>JSON</code> 的目标是 "轻量级数据交换格式", 同时具有 "易于人类读写", "易于代码解析和生成" 的特点。它在对 "人类而言的便利性" 和 "对机器而言的便利性" 之间取得了较好的平衡, 在许多应用场景下都是比 <code>XML</code> 更好的替代方案。</p>
<p>然而，将 <code>JSON</code> 用于其他目的有点类似于说 "嘿，这把锤子非常适合钉钉子！我喜欢它！为什么不用它来拧螺丝！" 当然它不是完全不能用，只是不合适做这样的工作。</p>
<p>目前为止，将 <code>JSON</code> 用作其它用途最大的问题在于不能在 <code>JSON</code> 文件中添加注释。某些特定的 <code>JSON</code> 解析器支持在 <code>JSON</code> 中添加注释，但是绝大部分的解析器都不支持。<code>JSON</code> 的发明者 <code>Douglas Crockford</code> 声称 <code>JSON</code> 最开始是支持注释的，然而由于一些原因，他特意移除了对注释的支持。想要深入研究的朋友可以看<a href="https://vorba.ch/2013/json-comments.html" target="_blank" rel="noopener noreferrer" title="Why are comments not allowed in JSON?">这里</a>。</p>
<p>我们在写配置文件时经常会遇到需要添加注释的场景。例如解释为什么将配置项设置为当前的值，添加一些助记符或是注意事项，对于错误配置的警告，在文件中保存一份基础的 <code>changelog</code>，又或单纯是在debug时需要注释掉一些配置项。</p>
<p>一个可行的解决方法是将原本的数据存储在一个 object 中，在这个 object 中通过两个条目分别存储数据和注释。例如原本的配置文件如下：</p>
<div class="language-json codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-json codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">"config_name"</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">"config_value"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">}</span><br></span></code></pre></div></div>
<p>修改后变成如下形式:</p>
<div class="language-json codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-json codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">"config_name"</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	  </span><span class="token property" style="color:#36acaa">"actual_data"</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">"config_value"</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">		</span><span class="token property" style="color:#36acaa">"comment"</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">"a comment"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token punctuation" style="color:#393A34">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">}</span><br></span></code></pre></div></div>
<p>但是在我看来这种方式丑的压批。</p>
<p>还有一些人指出可以通过 commit log 的形式来实现注释 <em>（译者：不清楚他这里指的是不是 git commit log，如果是的话把这个当作注释方式好像十分难用吧？）</em>，但是又有几个人会去细读 commit history？</p>
<p>一些基于 <code>JSON</code> 进行扩展的格式，例如 <code>JSON5</code>，<code>Hjson</code> 和 <code>HOCON</code>，以及一小部分 <code>JSON</code> 解析器添加了对注释的支持。这很实用，但这些都属于 <code>JSON</code> 的变种，因此不在本篇的讨论范围之内。</p>
<p>同时我也发现手工编辑 <code>JSON</code> 的用户体验不是那么友好：你得留意行尾是否要添加逗号，得了解用不用引号对含义的影响，同时 <code>JSON</code> 也不支持字符串内换行。这些特性对于 "轻量级数据交换格式" 而言不是坏事，但是对于编辑配置文件这件事来说却不是那么可爱。总的来说，将 <code>JSON</code> 用作配置文件虽然可行，但并不优雅。</p>
<p>MediaWiki 的新扩展系统促使我写下这篇文章。旧的系统通过 PHP 文件来挂接核心代码，加载所需的依赖项等。新系统通过 JSON 文件实现这些配置。这样的更新损失了 PHP 那种能够巧妙解决与其他插件兼容性的能力。 <em>（这段没看懂）</em></p>
<p>同时它也带来了更多实现复杂度。旧的系统在引入配置文件时仅仅需要一行代码：</p>
<div class="language-javascript codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-javascript codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token function" style="color:#d73a49">require</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">'plugin/foo/plugin.php'</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">;</span><br></span></code></pre></div></div>
<p>而新系统却需要对 JSON 文件的内容进行解析。这在提升实现复杂度的同时，也提高了 debug 的难度。
<em>（这段不太赞同，XML 作为配置文件，同样要进行解析，这不是 JSON 的问题。）</em></p>
<p>使用 JSON 文件存储基本元数据是可行的（更容易解析以及在网站上显示），但使用它来描述代码的工作方式对我来说是滥用 DC（Declarative configuration ，声明性配置）。毕竟，这是代码的工作。</p>
<p>许多人问我那到底该用什么(来做配置文件)，这其实是个很复杂的问题，关系到你程序的应用场景、编程语言、库环境甚至一些社交因素（？）。最好的回答可能就是“找到能符合你需求的最简单的方案”。</p>
<p>有一些 JSON 的扩展格式更适合于人类进行编辑，例如 JSON5、Hjson 和 HOCON。这些看起来都是普通JSON的合理升级，尽管我自己没有使用过它们。特别是 JSON5 似乎是一个不错的选择，因为它对 JSON 的改动最少。我不能给出关于这些扩展格式的建议，因为我没有所有的格式进行深入的比较。只是看一眼格式规范并不能发现潜在的缺点（YAML 就是一个很好的例子）。我没有时间或是兴趣对所有替代方案进行全面深入的审查。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="后记">后记<a href="https://edwardzjl.github.io/the-downsides-of-json-for-config-files#%E5%90%8E%E8%AE%B0" class="hash-link" aria-label="后记的直接链接" title="后记的直接链接">​</a></h2>
<p>这是我第一次做需要发布到网上的比较正式的翻译工作。虽然最早自己在读 paper 的时候因为英语生疏，也会边读边翻译一些，但是毕竟那是翻译给自己看的，只要自己能看懂就行了，也不用追求什么语句通顺之类的。然而要发布出来的文章不一样，至少要保证大多数读者能够看得懂。</p>
<p>整篇翻完回过头看看，还是有很多生硬似机翻的地方，主要原因可能还是自己的表达能力不够。翻译技术文章在我看来是个吃力不讨好的活，翻的再好也不如直接读原文来的清晰。至于为什么要做这样的事情， 我想有时间单独写一篇谈一谈。目前来看，就权当是对于自己表达能力的锻炼吧。</p>]]></content:encoded>
            <category>json</category>
        </item>
        <item>
            <title><![CDATA[系统中状态为 static 的服务]]></title>
            <link>https://edwardzjl.github.io/static-service</link>
            <guid>https://edwardzjl.github.io/static-service</guid>
            <pubDate>Thu, 04 Jul 2019 00:00:00 GMT</pubDate>
            <description><![CDATA[最近开始接触 Linux 运维的工作，第一件事情就是看看系统中跑了多少服务。]]></description>
            <content:encoded><![CDATA[<p>最近开始接触 Linux 运维的工作，第一件事情就是看看系统中跑了多少服务。</p>
<p>集群用的是 CentOS 7，可以通过 <code>bash systemctl list-unit-files</code> 这个命令查看所有服务，敲下回车后打印出来这么一堆玩应儿：</p>
<p><img decoding="async" loading="lazy" alt="services" src="https://edwardzjl.github.io/assets/images/services-1f32b5744640cabbd42f360a89b1bffb.png" title="services" width="454" height="474" class="img_ev3q"></p>
<p>service 的 <code>disabled</code> 和 <code>enabled</code> 状态都好理解，<code>static</code> 是个啥？在<a href="https://bbs.archlinux.org/viewtopic.php?id=147964" target="_blank" rel="noopener noreferrer" title="systemd 'static' unit file state">不存在的网站</a>上一顿查找，找到如下这番解释：</p>
<blockquote>
<p>"static" means "enabled because something else wants it". Think by analogy to pacman's package install reasons:</p>
<ul>
<li>enabled :: explicitly installed</li>
<li>static :: installed as dependency</li>
<li>disabled :: not installed</li>
</ul>
</blockquote>
<p>意思是，状态为 <code>static</code> 的服务，是作为别的服务的依赖而存在。</p>]]></content:encoded>
            <category>linux</category>
        </item>
        <item>
            <title><![CDATA[[译] javax.persistence.Id 和 org.springframework.data.annotation.Id 的区别]]></title>
            <link>https://edwardzjl.github.io/difference-between-javax.persistence.id-and-org.springframework.data.annotation.id</link>
            <guid>https://edwardzjl.github.io/difference-between-javax.persistence.id-and-org.springframework.data.annotation.id</guid>
            <pubDate>Thu, 27 Jun 2019 00:00:00 GMT</pubDate>
            <description><![CDATA[org.springframework.data.annotation.Id]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="orgspringframeworkdataannotationid">org.springframework.data.annotation.Id<a href="https://edwardzjl.github.io/difference-between-javax.persistence.id-and-org.springframework.data.annotation.id#orgspringframeworkdataannotationid" class="hash-link" aria-label="org.springframework.data.annotation.Id的直接链接" title="org.springframework.data.annotation.Id的直接链接">​</a></h2>
<p><code>org.springframework.data.annotation.Id</code> 是 Spring 定义的 annotation，用来支持 "没有像 JPA 那样的持久化 API" 的非关系型数据库或是框架的持久化，因此它常被用于其它 spring-data 项目，例如 spring-data-mongodb 和 spring-data-solr 等。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="javaxpersistenceid">javax.persistence.Id<a href="https://edwardzjl.github.io/difference-between-javax.persistence.id-and-org.springframework.data.annotation.id#javaxpersistenceid" class="hash-link" aria-label="javax.persistence.Id的直接链接" title="javax.persistence.Id的直接链接">​</a></h2>
<p><code>javax.persistence.Id</code> 是由 JPA 定义的 annotation，JPA 仅适用于关系数据的管理。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="ref">Ref<a href="https://edwardzjl.github.io/difference-between-javax.persistence.id-and-org.springframework.data.annotation.id#ref" class="hash-link" aria-label="Ref的直接链接" title="Ref的直接链接">​</a></h2>
<ul>
<li><a href="https://stackoverflow.com/questions/39643960/whats-the-difference-between-javax-persistence-id-and-org-springframework-data" target="_blank" rel="noopener noreferrer">whats-the-difference-between-javax-persistence-id-and-org-springframework-data</a></li>
</ul>]]></content:encoded>
            <category>spring</category>
            <category>java</category>
        </item>
        <item>
            <title><![CDATA[Install postgres on OSX]]></title>
            <link>https://edwardzjl.github.io/install-postgres-on-osx</link>
            <guid>https://edwardzjl.github.io/install-postgres-on-osx</guid>
            <pubDate>Sat, 13 Apr 2019 00:00:00 GMT</pubDate>
            <description><![CDATA[If you installed Postgres from homebrew, the default user postgres isn't automatically created, you need to run following command in your terminal:]]></description>
            <content:encoded><![CDATA[<p>If you installed Postgres from homebrew, the default user <code>postgres</code> isn't automatically created, you need to run following command in your terminal:</p>
<div class="language-sh codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-sh codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">/Applications/Postgres.app/Contents/Versions/9.*/bin/createuser -s postgres</span><br></span></code></pre></div></div>]]></content:encoded>
            <category>postgres</category>
            <category>osx</category>
        </item>
        <item>
            <title><![CDATA[Ubuntu / CentOS 编译安装带剪贴板支持的 Vim]]></title>
            <link>https://edwardzjl.github.io/config-vim-8-clipboard</link>
            <guid>https://edwardzjl.github.io/config-vim-8-clipboard</guid>
            <pubDate>Thu, 14 Mar 2019 00:00:00 GMT</pubDate>
            <description><![CDATA[通过 Ubuntu 或 CentOS 系统自带的软件源安装 Vim，往往只能得到较旧的版本（通常是 7.4.x）。而从 Vim 8.0 开始，官网推荐的安装方式是通过 Git 克隆源码自行编译。]]></description>
            <content:encoded><![CDATA[<p>通过 Ubuntu 或 CentOS 系统自带的软件源安装 Vim，往往只能得到较旧的版本（通常是 7.4.x）。而从 Vim 8.0 开始，官网推荐的安装方式是通过 Git 克隆源码自行编译。</p>
<p>不过需要注意，<strong>默认编译出来的 Vim 并不包含剪贴板支持（clipboard support）</strong>，因此无法与系统剪贴板交互（例如复制粘贴到其他程序）。</p>
<p>要在编译时启用剪贴板支持，至少需要两个依赖包：</p>
<ul>
<li><code>libx11-dev</code>：提供 Xorg 的头文件（xorg header files）</li>
<li><code>dbus-x11</code>：提供 X11 的 D-Bus 支持</li>
</ul>
<p>你可以在 <a href="https://packages.ubuntu.com/" target="_blank" rel="noopener noreferrer">https://packages.ubuntu.com</a> 搜索具体的依赖项位置，最终确认这两个包就是我们所需的。</p>
<p>安装依赖并编译 Vim 的完整流程如下：</p>
<div class="language-sh codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-sh codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">sudo apt-get install libx11-dev dbus-x11</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">git clone https://github.com/vim/vim.git</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cd vim</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">./configure --with-features=huge --enable-gui=auto --enable-cscope --prefix=/usr/local</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">make</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">sudo make install</span><br></span></code></pre></div></div>
<blockquote>
<p>其中 <code>--with-features=huge</code> 启用几乎所有功能，<code>--enable-gui=auto</code> 可选启用 GUI 模式（如 gvim），<code>--enable-cscope</code> 则用于增强代码导航功能。</p>
</blockquote>
<p>安装完成后，你可以使用 <code>vim --version</code> 检查是否启用了 <code>+clipboard</code>，确认剪贴板支持是否生效。</p>]]></content:encoded>
            <category>vim</category>
        </item>
    </channel>
</rss>