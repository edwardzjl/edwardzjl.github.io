"use strict";(self.webpackChunkedwardzjl_github_io=self.webpackChunkedwardzjl_github_io||[]).push([[165],{1812:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>c});var s=t(7308),i=t(4848),r=t(8453);const a={slug:"distributed-sft-part-1",authors:["jlzhou"],tags:["LLM","distributed-training"]},o="Distributed SFT Part 1: Starting Locally",l={authorsImageUrls:[void 0]},c=[{value:"Introduction",id:"introduction",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Training",id:"training",level:2},{value:"Command-line Arguments",id:"command-line-arguments",level:3},{value:"The Oops",id:"the-oops",level:3},{value:"The Fix",id:"the-fix",level:3},{value:"Conclusion",id:"conclusion",level:2}];function d(e){const n={a:"a",blockquote:"blockquote",code:"code",em:"em",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsxs)(n.p,{children:["Welcome to this series of articles documenting the lessons I learned during my first attempt at running distributed supervised fine-tuning (SFT) tasks using ",(0,i.jsx)(n.a,{href:"https://github.com/huggingface/trl",children:"trl"})," and ",(0,i.jsx)(n.a,{href:"https://github.com/microsoft/DeepSpeed",children:"DeepSpeed"}),"."]}),"\n",(0,i.jsx)(n.p,{children:"This series will walk you through my journey, starting with a simple local experiment and progressively scaling up to a distributed environment. The three parts of this series are:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Part 1: The Local Experiment"})," -- I will show you how I ran my very first local SFT experiment, following the official ",(0,i.jsx)(n.a,{href:"https://huggingface.co/docs/trl/sft_trainer",children:"trl documentation"}),"."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Part 2: Multi GPU"})," -- We will leverage ",(0,i.jsx)(n.strong,{children:"single-machine, multi-GPU"})," parallel training to complete a full SFT task in our local environment."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Part 3: Multi Machine"})," -- We'll take things a step further by submitting the same training task to a Kubernetes cluster, utilizing ",(0,i.jsx)(n.strong,{children:"multi-machine, multi-GPU"})," training with ",(0,i.jsx)(n.a,{href:"https://github.com/kubeflow/training-operator",children:"Kubeflow's Training Operator"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"A quick note about myself: I'm a software development engineer who is fairly new to the field of deep learning. If these articles seem too basic for you, I appreciate your patience as I navigate this learning journey."}),"\n",(0,i.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsxs)(n.p,{children:["To follow this tutorial, you'll need a machine with at least one NVIDIA GPU. I ran the experiment on a V100 without encountering any memory issues. If your GPU has less than 32GB of VRAM, you may need to reduce the ",(0,i.jsx)(n.code,{children:"per_device_train_batch_size"})," or consider using truncation (although this is not recommended) to prevent CUDA out-of-memory (OOM) errors."]}),"\n",(0,i.jsx)(n.p,{children:"You'll also need the following dependencies:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-txt",children:"datasets\ntransformers\ntrl\ntorch\n"})}),"\n",(0,i.jsx)(n.h2,{id:"training",children:"Training"}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"trl"})," library offers some excellent example training scripts, and we'll start with this one: ",(0,i.jsx)(n.a,{href:"https://github.com/huggingface/trl/blob/main/trl/scripts/sft.py",children:"https://github.com/huggingface/trl/blob/main/trl/scripts/sft.py"})]}),"\n",(0,i.jsxs)(n.p,{children:["Copy the script to your development machine (or notebook), select a base model, and pick an SFT dataset to run the experiment. For this experiment, I chose ",(0,i.jsx)(n.a,{href:"https://huggingface.co/Qwen/Qwen2.5-0.5B",children:"Qwen/Qwen2.5-0.5B"})," as the base model for its compact size, and ",(0,i.jsx)(n.a,{href:"https://huggingface.co/datasets/BAAI/Infinity-Instruct",children:"BAAI/Infinity-Instruct"})," as the SFT dataset (somehow randomly \ud83d\ude0c). You can explore other interesting datasets here: ",(0,i.jsx)(n.a,{href:"https://github.com/mlabonne/llm-datasets",children:"https://github.com/mlabonne/llm-datasets"}),"."]}),"\n",(0,i.jsx)(n.h3,{id:"command-line-arguments",children:"Command-line Arguments"}),"\n",(0,i.jsxs)(n.p,{children:["The training script (",(0,i.jsx)(n.code,{children:"sft.py"}),") exposes a variety of useful command-line arguments that allow you to customize the fine-tuning process. These arguments are mapped to specific properties in the following classes:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://huggingface.co/docs/trl/v0.13.0/en/script_utils#trl.ScriptArguments",children:"ScriptArguments"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://github.com/huggingface/trl/blob/v0.13.0/trl/trainer/model_config.py#L20",children:"ModelConfig"})}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://huggingface.co/docs/trl/v0.13.0/en/sft_trainer#trl.SFTConfig",children:"SFTConfig"}),", which extends ",(0,i.jsx)(n.a,{href:"https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments",children:"TrainingArguments"})]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["You can pass any of these arguments directly from the command line by prepending them with ",(0,i.jsx)(n.code,{children:"--"}),". For instance, passing ",(0,i.jsx)(n.code,{children:"--dataset_name"})," will set the ",(0,i.jsx)(n.code,{children:"dataset_name"})," field in the ",(0,i.jsx)(n.code,{children:"trl.ScriptArguments"})," class."]}),"\n",(0,i.jsx)(n.p,{children:"Let's take a look at the arguments used for this tutorial:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"--model_name_or_path"}),": Specifies the base model to fine-tune."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"--dataset_name"}),": Defines the dataset to use for fine-tuning."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"--dataset_config"}),": Some datasets come with multiple configurations (versions). This argument lets you choose the version you want to use."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"--do_train"}),": Tells the script to start the training process."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"--per_device_train_batch_size"}),": Defines the batch size for each GPU."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"--output_dir"}),": Specifies the directory where the model will be saved."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"--max_steps"}),": Sets the maximum number of training steps."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"--logging_steps"}),": Sets how often logs are recorded during training."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"For convenience, I prefer to save the full command in a shell script for easy execution. Here's the script I used for this tutorial:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sh",children:"python sft.py \\\n  --model_name_or_path Qwen/Qwen2.5-0.5B \\\n  --dataset_name BAAI/Infinity-Instruct \\\n  --dataset_config 0625 \\\n  --do_train \\\n  --per_device_train_batch_size 4 \\\n  --output_dir /tmp/my-first-sft-exp \\\n  --max_steps 10 \\\n  --logging_steps 1\n"})}),"\n",(0,i.jsx)(n.p,{children:"Notes:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"I selected the smallest version of the dataset and limited the experiment to just 10 steps for a quicker run."}),"\n",(0,i.jsxs)(n.li,{children:["Since the training is only 10 steps, I set ",(0,i.jsx)(n.code,{children:"--logging_steps"})," to 1 to see logs more frequently."]}),"\n",(0,i.jsxs)(n.li,{children:["The ",(0,i.jsx)(n.code,{children:"--per_device_train_batch_size"})," is set to 4, as the goal here isn't model quality but simply to run the experiment without CUDA OOM. Any number that can fit in your VRAM should work."]}),"\n"]}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.em,{children:"Updated 2025-02-18:"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"trl"})," provides a convenient helper function to parse training args from a YAML file, you can find more details ",(0,i.jsx)(n.a,{href:"https://huggingface.co/docs/trl/main/script_utils#trl.TrlParser.parse_args_and_config",children:"here"}),"."]}),"\n",(0,i.jsxs)(n.p,{children:["With this feature, you can save the above training arguments in a YAML file (e.g., ",(0,i.jsx)(n.code,{children:"recipe.yaml"}),") as follows:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:"model_name_or_path: Qwen/Qwen2.5-0.5B\ndataset_name: BAAI/Infinity-Instruct\ndataset_config: '0625'\ndo_train: true\nper_device_train_batch_size: 4\noutput_dir: /tmp/my-first-sft-exp\nmax_steps: 10\nlogging_steps: 1\n"})}),"\n",(0,i.jsx)(n.p,{children:"And launch the training with:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sh",children:"python sft.py --config recipe.yaml\n"})}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"the-oops",children:"The Oops"}),"\n",(0,i.jsx)(n.p,{children:"Now if you use the same dataset and execute the same script, you'll likely encounter a (not so helpful) error message:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-console",children:'$ ./quickstart.sh \nResolving data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 35/35 [00:00<00:00, 50.35it/s]\nMap:   0%|                                                                                                                                                                         | 0/659808 [00:00<?, ? examples/s]\nTraceback (most recent call last):\n  File "/home/jovyan/sft-walkthrough/sft.py", line 126, in <module>\n    main(script_args, training_args, model_args)\n  File "/home/jovyan/sft-walkthrough/sft.py", line 97, in main\n    trainer = SFTTrainer(\n  ...\n  File "/home/jovyan/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 416, in tokenize\n    element[dataset_text_field] if formatting_func is None else formatting_func(element),\n  File "/home/jovyan/.local/lib/python3.10/site-packages/datasets/formatting/formatting.py", line 277, in __getitem__\n    value = self.data[key]\nKeyError: \'text\'\n'})}),"\n",(0,i.jsx)(n.h3,{id:"the-fix",children:"The Fix"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.em,{children:"Updated 2025-02-18:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Starting from trl 0.15.0 (in ",(0,i.jsx)(n.a,{href:"https://github.com/huggingface/trl/pull/2405",children:"this PR"}),") the 'conversations' column is no longer supported. We need to rename it to 'messages'."]}),"\n",(0,i.jsxs)(n.li,{children:["In ",(0,i.jsx)(n.a,{href:"https://github.com/huggingface/trl/pull/2862",children:"this PR"})," (not yet released as of writing), support for the 'conversations' column is back and the whole preprocessing is simplified, we no longer need to map the dict keys('from' -> 'role', 'value' -> 'content') ourselves."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.em,{children:"Updated 2025-02-19:"})}),"\n",(0,i.jsx)(n.p,{children:"The above PR is released in trl 0.15.1."}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["This error message is a bit confusing--it states that the ",(0,i.jsx)(n.code,{children:"SFTTrainer"})," requires the dataset to have a 'text' field. However, according to the ",(0,i.jsx)(n.a,{href:"https://huggingface.co/docs/trl/dataset_formats#overview-of-the-dataset-formats-and-types",children:"dataset format and types"}),", 'text' is used for standard dataset, while 'messages' should be used for conversational datasets. After a lot of googling, I came across ",(0,i.jsx)(n.a,{href:"https://github.com/huggingface/trl/issues/2071",children:"this tracking issue"}),", ",(0,i.jsx)(n.a,{href:"https://github.com/huggingface/trl/blob/v0.13.0/trl/trainer/sft_trainer.py#L250",children:"this line of code"})," and ",(0,i.jsx)(n.a,{href:"https://github.com/huggingface/trl/blob/v0.13.0/trl/extras/dataset_formatting.py#L78",children:"this function"}),". It seems that for the current implementation (",(0,i.jsx)(n.code,{children:"trl == 0.13.0"}),") we have two options:"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Format the dataset ourselves (apply a chat template) and place the formatted data into the 'text' field."}),"\n",(0,i.jsxs)(n.li,{children:["Convert our dataset in a way that allows ",(0,i.jsx)(n.code,{children:"trl"})," to handle the transformation for us."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"For the second option to work, the dataset must:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Contain a 'messages' or 'conversations' field."}),"\n",(0,i.jsx)(n.li,{children:"Have each element in the 'messages' (or 'conversations') field include both a 'content' field and a 'role' field."}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Examining the dataset I used revealed a mismatch: while it has a 'conversations' field, the elements inside use 'from' and 'value' as keys instead of 'role' and 'content'. As a lazy coder, I opted for the second approach and updated the training script accordingly. Additionally, I also remove all other fields in the dataset, as they are unused for the SFT task. Removing them will slightly reduce memory footprint and speed up processing."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'...\ndef main(script_args, training_args, model_args):\n    ...\n    ################\n    # Dataset\n    ################\n    dataset = load_dataset(script_args.dataset_name, name=script_args.dataset_config)\n\n    def convert_fields(message: dict) -> dict:\n        _message = {\n          "role": message["from"],\n          "content": message["value"],\n        }\n        # Qwen2.5 tokenizer only supports "system", "user", "assistant" and "tool"\n        # See <https://huggingface.co/Qwen/Qwen2.5-3B/blob/main/tokenizer_config.json#L198>\n        if _message["role"] == "human":\n            _message["role"] = "user"\n        elif _message["role"] == "gpt":\n            _message["role"] = "assistant"\n        elif _message["role"] == "system":\n            # nothing to be done.\n            ...\n        else:\n            # In case there are any other roles, print them so we can improve in next iteration.\n            print(_message["role"])\n        return _message\n\n    def convert_messages(example):\n        example["conversations"] = [convert_fields(message) for message in example["conversations"]]\n        return example\n\n    # remove unused fields\n    dataset = dataset.remove_columns(["id", "label", "langdetect", "source"]).map(convert_messages)\n    ...\n'})}),"\n",(0,i.jsx)(n.p,{children:"With that update, the script ran without any issues! You should be able to see the training log like:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-console",children:"$ ./quickstart.sh \nResolving data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 35/35 [00:02<00:00, 17.26it/s]\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 659808/659808 [01:19<00:00, 8280.44 examples/s]\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 659808/659808 [08:33<00:00, 1284.45 examples/s]\n{'loss': 1.8859, 'grad_norm': 14.986075401306152, 'learning_rate': 1.8e-05, 'epoch': 0.0}                                                                                                                                     \n{'loss': 1.4527, 'grad_norm': 13.9092378616333, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.0}                                                                                                                        \n{'loss': 1.467, 'grad_norm': 7.388503074645996, 'learning_rate': 1.4e-05, 'epoch': 0.0}                                                                                                                                       \n{'loss': 1.7757, 'grad_norm': 9.457520484924316, 'learning_rate': 1.2e-05, 'epoch': 0.0}                                                                                                                                      \n{'loss': 1.9043, 'grad_norm': 10.256357192993164, 'learning_rate': 1e-05, 'epoch': 0.0}                                                                                                                                       \n{'loss': 1.6163, 'grad_norm': 10.774249076843262, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.0}                                                                                                                       \n{'loss': 1.1774, 'grad_norm': 5.897563457489014, 'learning_rate': 6e-06, 'epoch': 0.0}                                                                                                                                        \n{'loss': 1.8093, 'grad_norm': 8.3130464553833, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.0}                                                                                                                          \n{'loss': 1.8387, 'grad_norm': 7.102719306945801, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}                                                                                                                       \n{'loss': 1.4251, 'grad_norm': 9.853829383850098, 'learning_rate': 0.0, 'epoch': 0.0}                                                                                                                                          \n{'train_runtime': 38.8598, 'train_samples_per_second': 1.029, 'train_steps_per_second': 0.257, 'train_loss': 1.635251808166504, 'epoch': 0.0}                                                                                 \n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:38<00:00,  3.89s/it]\n"})}),"\n",(0,i.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,i.jsxs)(n.p,{children:["In this first part, we've walked through setting up a local SFT experiment using ",(0,i.jsx)(n.code,{children:"trl"}),". This library provides a user-friendly interface for fine-tuning LLMs with custom datasets. We also covered the correct dataset format required for ",(0,i.jsx)(n.code,{children:"trl"}),"'s ",(0,i.jsx)(n.code,{children:"SFTTrainer"})," and how to preprocess datasets to meet these requirements."]}),"\n",(0,i.jsx)(n.p,{children:"In the next part, we'll delve into scaling this setup locally using a single-node, multi-GPU configuration to tackle a complete SFT task. Additionally, we'll explore various optimization techniques to fit a bigger model into your GPU and accelerate the training process. Stay tuned!"})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},7308:e=>{e.exports=JSON.parse('{"permalink":"/en/distributed-sft-part-1","editUrl":"https://github.com/edwardzjl/edwardzjl.github.io/blob/main/blog/2025-01-23-distributed-sft-part-1/index.md","source":"@site/blog/2025-01-23-distributed-sft-part-1/index.md","title":"Distributed SFT Part 1: Starting Locally","description":"Introduction","date":"2025-01-23T00:00:00.000Z","tags":[{"inline":true,"label":"LLM","permalink":"/en/tags/llm"},{"inline":true,"label":"distributed-training","permalink":"/en/tags/distributed-training"}],"readingTime":7.93,"hasTruncateMarker":true,"authors":[{"name":"Junlin Zhou","title":"Fullstack Engineer @ ZJU ICI","url":"https://github.com/edwardzjl","imageURL":"https://github.com/edwardzjl.png","key":"jlzhou","page":null}],"frontMatter":{"slug":"distributed-sft-part-1","authors":["jlzhou"],"tags":["LLM","distributed-training"]},"unlisted":false,"nextItem":{"title":"<\u8bd1>JSON\u683c\u5f0f\u4f5c\u4e3a\u914d\u7f6e\u6587\u4ef6\u7684\u7f3a\u70b9","permalink":"/en/the-downsides-of-json-for-config-files"}}')},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>o});var s=t(6540);const i={},r=s.createContext(i);function a(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);